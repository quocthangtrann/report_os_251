\documentclass[a4paper,13pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{array}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{csvsimple}
\usepackage[T1]{fontenc}
\usepackage{ragged2e} 
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{references.bib}
\usepackage{setspace} 
\usepackage{algorithm}
\usepackage{pgfplots}
\usepgfplotslibrary{patchplots}
\usepackage{pgfgantt}
\usepackage{array}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{verbatim}
\usepackage{pgfplots}
\usepackage{tikz-3dplot}
\usepackage{lastpage}
\usepackage{longtable}
\usepackage{framed}  
\addbibresource{references.bib}
\usepackage{enumitem}

\lstset{
    language=C,
    basicstyle=\ttfamily\small, % Font chữ code
    numbers=left,               % Số dòng bên trái
    numberstyle=\tiny\color{gray}, % Style số dòng
    stepnumber=1,               % Đánh số từng dòng
    numbersep=8pt,              % Khoảng cách số dòng và code
    backgroundcolor=\color{white},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,               % TẠO KHUNG VIỀN (FRAMED)
    rulecolor=\color{black},    % Màu khung
    tabsize=4,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    title=\lstname,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\geometry{
  a4paper,
  total={170mm,257mm},
  left=20mm,
  right=20mm,
  top=20mm,
  bottom=20mm,
}
\renewcommand{\abstractname}{\textbf{\Large\uppercase{Abstract}}}

\everymath{\displaystyle}

\AtBeginDocument{
  \abovedisplayskip=6pt plus 2pt minus 4pt
  \belowdisplayskip=6pt plus 2pt minus 4pt
  \abovedisplayshortskip=0pt plus 2pt
  \belowdisplayshortskip=4pt plus 2pt minus 2pt
}

% Thiết lập header và footer
\pagestyle{fancy}
\fancyhf{} % Xóa các kiểu header và footer mặc định

% Header
\fancyhead[L]{\includegraphics[height=1.5cm]{logobk.png}} 
\fancyhead[C]{\textbf{VIETNAM NATIONAL UNIVERSITY HO CHI MINH CITY\\HO CHI MINH CITY UNIVERSITY OF TECHNOLOGY}}
\fancyhead[R]{}

% Footer
\fancyfoot[C]{\textbf{Page} \thepage\ \textbf{|} \pageref{LastPage}}

\setlength{\headheight}{28pt} % Tăng chiều cao của header để tránh chồng lấn
\setlength{\headsep}{30pt}    % Tạo khoảng cách giữa header và nội dung

% Bỏ header và footer ở trang đầu tiên nếu cần
\renewcommand{\headrulewidth}{0.5pt} % Đường kẻ ở header
\renewcommand{\footrulewidth}{0.5pt} % Đường kẻ ở footer


\begin{document}


\begin{titlepage}
\definecolor{myblue1}{RGB}{3, 43, 145}
\definecolor{myblue2}{RGB}{30, 126, 219}

\begin{tikzpicture}[remember picture, overlay]
  \draw[myblue1, thick] ([shift={(0.8cm, -0.8cm)}]current page.north west) rectangle ([shift={(-0.8cm, 0.8cm)}]current page.south east);
  \draw[myblue2, thick]([shift={(1cm, -1cm)}]current page.north west) rectangle ([shift={(-1cm, 1cm)}]current page.south east);
\end{tikzpicture}

\begin{center}
  \textbf{ VIETNAM NATIONAL UNIVERSITY HO CHI MINH CITY}\\
  \vspace{0.1cm}
  \textbf{HO CHI MINH CITY UNIVERSITY OF TECHNOLOGY}\\
  \vspace{0.1cm}
  \textbf{ FACULTY OF COMPUTER SCIENCE AND ENGINEERING}\\
  \vspace{2.5cm}
      \centering
      \includegraphics[scale = 1.0]{logobk.png} \\
    \vspace{1.0cm}

  \textbf{\fontsize{30pt}{0pt}\LARGE REPORT ASSIGNMENT}\\
  \vspace{1.0cm}    
  \textbf{\LARGE Operating System}\\
  \vspace{2cm}
  \textbf{Class: CC01 - HK251}\\
  \textbf{INSTRUCTOR: NGUYEN PHUONG DUY}\\
    \vspace{1cm}
  \textbf{TOPIC: DESIGN SIMPLE OPERATING SYSTEM}\\
  \vspace{1cm}
  
  {\large
			\begin{tabular}{|c|c|}
				\hline
				\textbf{student's name} & \textbf{Student's ID} \\ \hline
				Le Phuc Khang & 2352471 \\ \hline
				Tran Quoc Thang & 2353125 \\ \hline
				Tran Xuan Hao & 2252191 \\ \hline
				Nguyen Tuan Ngoc & 2352815 \\ \hline
				Phan Tuan Kiet & 2352654 \\ \hline
			\end{tabular}
    }
  
  \vspace{4cm}
  \textbf{Ho Chi Minh City, 2025}
\end{center}

\end{titlepage}

\thispagestyle{empty}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
  
    \textbf{ GROUP WORK RESULTS REPORT} \\
    \vspace{0.5cm}
    \begin{tabular}{|c|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{3cm}|} \hline
      \textbf{Number} & \textbf{Full name} & \textbf{Student ID} & \textbf{Contribution} \\ \hline
      1 & Le Phuc Khang & 2352471 & 100\% \\ \hline
      2 & Tran Quoc Thang & 2353125 & 100\% \\ \hline
      3 & Nguyen Tuan Ngoc & 2352815 & 100\% \\ \hline
      4 & Phan Tuan Kiet & 2352654 & 100\% \\ \hline
      5 & Tran Xuan Hao & 2252191 & 100\% \\ \hline
    \end{tabular} \\
    \end{center}
    
    \vspace{0.5cm}


\newpage

\justifying % Căn đều đoạn văn bản dưới đây
\setstretch{1.5} % Thiết lập khoảng cách giữa các dòng

\tableofcontents

\pagebreak

\section{Theorical Background}

\subsection{Scheduler}

The scheduler is the central component of the kernel responsible for deciding which process runs at any given time. Our implementation is grounded in the theoretical framework of CPU Scheduling described in Chapter 5 of \textit{Operating System Concepts}. Specifically, we construct a \textbf{Symmetric Multiprocessing (SMP)} system utilizing a \textbf{Preemptive Multilevel Queue (MLQ)} algorithm with a \textbf{Time-Slicing allocation strategy} to address starvation.

\begin{enumerate}
    \item \textbf{Multilevel Queue Scheduling (Theory of Partitioning):}
    \begin{itemize}
        \item \textbf{Theoretical Basis:} Processes in a system often have different response-time requirements (e.g., interactive/foreground vs. batch/background). A Multilevel Queue (MLQ) algorithm partitions the Ready Queue into several separate queues, each assigned to a different class of processes.
        \item \textbf{System Design:} In our assignment, we define \texttt{MAX\_PRIO} distinct queues. Processes are permanently assigned to a specific queue based on their \texttt{priority} attribute. Unlike Multilevel Feedback Queues (MLFQ), which allow processes to move between queues to define their behavior dynamically, our standard MLQ implementation reduces runtime overhead ($\mathcal{O}(1)$ queue selection) by maintaining fixed process classification.
    \end{itemize}

    \item \textbf{Intra-Queue Scheduling: Round-Robin (RR) and Preemption:}
    \begin{itemize}
        \item \textbf{Theoretical Basis:} To support time-sharing, Round-Robin (RR) scheduling is theoretically defined as FCFS scheduling with \textbf{Preemption}. A small unit of time, called a \textit{Time Quantum}, is defined. If a process's CPU burst exceeds this quantum, the system timer generates an interrupt, causing the OS to preempt the process and move it to the tail of the ready queue.
        \item \textbf{System Design:} We implement this by assigning a \texttt{time\_slot} to the system. The \texttt{queue.c} module strictly enforces FIFO behavior (\texttt{dequeue} from head, \texttt{enqueue} to tail), which, combined with the timer interrupt in \texttt{os.c}, realizes the Round-Robin logic. This ensures \textbf{fairness} among processes of the same priority and minimizes the average response time.
    \end{itemize}

    \item \textbf{Inter-Queue Scheduling: Solving Starvation via Time-Slicing:}
    \begin{itemize}
        \item \textbf{The Starvation Problem:} A fundamental flaw of fixed-priority scheduling is \textbf{Indefinite Blocking} (or Starvation), where low-priority tasks may never execute if a continuous stream of high-priority tasks arrives.
        \item \textbf{Theoretical Solution:} The textbook proposes two solutions: \textit{Aging} or \textit{Time Slicing among queues}. In the Time Slicing approach, each queue gets a certain portion of the CPU time (e.g., 80\% for foreground, 20\% for background).
        \item \textbf{System Design (Slot Mechanism):} We implement the \textbf{Time Slicing} solution using a deterministic slot mechanism. Each queue $i$ is allocated a specific budget: $slot[i] = MAX\_PRIO - i$. 
        The scheduler iterates through queues; if a high-priority queue consumes its allocated slots, the scheduler is theoretically forced to "yield" to lower-priority queues, even if the high-priority queue is non-empty. This converts the algorithm from "Absolute Priority" to "Proportional Share," mathematically guaranteeing that even the lowest priority queue eventually receives CPU cycles.
    \end{itemize}

  \item \textbf{Symmetric Multiprocessing (SMP) and Concurrency Control:}
    \begin{itemize}
        \item \textbf{Theoretical Basis:} According to the theory of \textit{Multiple-Processor Scheduling}, modern systems predominantly use Symmetric Multiprocessing (SMP), where each processor is self-scheduling. The literature outlines two primary strategies for organizing threads: (1) Per-processor private queues, and (2) A single \textbf{Common Ready Queue}. While private queues offer cache efficiency, the Common Ready Queue approach provides a unified pool of work but introduces specific synchronization challenges.
        
        \item \textbf{System Design (Load Balancing):} We adopt the \textbf{Common Ready Queue} architecture. As noted in the theoretical framework, this design naturally handles the distribution of workloads. Since all processors examine the same global queue to select a thread, the system inherently avoids the workload disparity often found in private-queue architectures. No complex balancing algorithms are needed because no processor remains idle while tasks exist in the global pool.
        
        \item \textbf{Synchronization Theory:} The textbook explicitly warns that a shared ready queue creates a potential \textbf{Race Condition}, where two separate processors might attempt to schedule the same thread simultaneously or corrupt the queue structure. To mitigate this, as mandated by the principles of concurrency control, we implement a locking mechanism (\texttt{pthread\_mutex\_t}). This lock protects the critical section (the shared queue), ensuring that enqueue and dequeue operations are strictly serialized, albeit at the cost of potential lock contention.
    \end{itemize}
\end{enumerate}

\subsection{Memory Management}

\subsubsection{Memory management}

The fundamental task of an operating system is to manage the memory hierarchy and provide an abstraction mechanism to separate the user process from the physical details of the hardware.
\paragraph{a. Logical vs. Physical Address Space}

The central concept of memory management is the separation of logical address space from physical address space:

\begin{itemize}
    \item \textbf{Logical Address:} An address generated by the CPU during program execution. To a process, memory appears as a flat, continuous space ranging from address $0$ to $Max_{Virtual}$. The set of all valid logical addresses for a process is defined as the \textit{Logical Address Space}.
    \item \textbf{Physical Address:} The actual address seen and accessed by the memory unit on the physical RAM.
    \item \textbf{Memory Management Unit (MMU):} A hardware device responsible for the run-time mapping from virtual to physical addresses. Under this scheme, the user program never interacts with the real physical addresses.
\end{itemize}


\paragraph{b. Process Memory Layout}
Although the virtual address space is linear, logically a process is typically organized into distinct sections to separate access rights and usage purposes:

\begin{itemize}
    \item \textbf{Text section:} contains the executable code of the program (usually read-only).
    \item \textbf{Data section:} stores global and static variables.
    \item \textbf{Heap:} region for dynamic allocations that conceptually grows upward.
    \item \textbf{Stack:} holds stack frames and local variables, growing downward.
\end{itemize}

In full-featured UNIX-like systems, each of the above logical sections is usually represented by one or more Virtual Memory Areas (VMAs) maintained in a linked list inside the process's \texttt{mm\_struct}.

In our simple OS implementation, we adopt a slightly different but simpler model. Each process owns a single user \texttt{vm\_area\_struct} that covers its whole logical user space (\texttt{vmaid = 0}). Inside this VMA, we represent individual logical ``segments'' by region descriptors \texttt{vm\_rg\_struct}, whose start and end addresses (\texttt{rg\_start}, \texttt{rg\_end}) are recorded in the symbol region table \texttt{symrgtbl[]}. Free holes between regions are maintained in the VMA's \texttt{vm\_freerg\_list}.

Therefore, in this project:
\begin{itemize}
    \item segment $\approx$ \texttt{vm\_rg\_struct} region inside one VMA,
    \item while the VMA itself simply defines the outer bounds of the user address space for the process.
\end{itemize}


\subsubsection{Multi-level paging}

To implement memory mapping in modern architectures, \textbf{Paging} is widely used to eliminate external fragmentation.

\paragraph{a. The Page Table Problem in 64-bit Systems}
In traditional 32-bit architectures, a two-level paging scheme is often sufficient. However, with a 64-bit architecture ($2^{64}$ bytes of address space), the size of the page table becomes prohibitively large if stored contiguously.

According to \textbf{Chapter 9.4.2 (Hierarchical Paging)}, the optimal solution is to divide the page table into smaller pieces. This allows the operating system to avoid allocating the entire page table structure in contiguous memory. Only the necessary sub-tables are allocated (on-demand allocation).

\paragraph{b. 5-Level Paging Architecture}
The system is designed based on an extension of the Intel x86-64 architecture, utilizing a \textbf{5-level paging scheme} to support a significantly larger virtual address space (up to 57-bit linear address).

The virtual address structure is decomposed into index fields and an offset. Assuming a standard page size of 4KB ($2^{12}$ bytes), the virtual address is resolved as follows:

\begin{equation}
    \texttt{Virtual\_Address} = PGD \oplus P4D \oplus PUD \oplus PMD \oplus PT \oplus Offset
\end{equation}

The address translation process (also known as the \textbf{Page Walk}) occurs sequentially through 5 tables:
\begin{enumerate}
    \item \textbf{PGD (Page Global Directory):} The top-level table (Level 5).
    \item \textbf{P4D (Page Level 4 Directory):} The Level 4 table.
    \item \textbf{PUD (Page Upper Directory):} The Level 3 table.
    \item \textbf{PMD (Page Middle Directory):} The Level 2 table.
    \item \textbf{PT (Page Table):} The final level (Level 1), containing the mapping to the physical frame.
\end{enumerate}

Each entry in the upper-level tables points to the base address of the next lower-level table. The entry in the \texttt{PT} (Page Table Entry - PTE) contains the actual \textbf{Physical Frame Number (PFN)}.

\subsubsection{Comparative Analysis of Page Table Architectures}

\noindent\textbf{a. Hierarchical Paging}

When applying Single-level paging on a 64-bit system with a 4KB ($2^{12}$) page size, the virtual address space contains approximately $2^{52}$ pages. If a single page table were used, the Operating System would be required to allocate a massive, contiguous block of physical memory to store all Page Table Entries for each process. This is practically impossible due to the sheer size exceeding RAM limits and the difficulty of finding contiguous memory caused by fragmentation.

Hierarchical Paging resolves this by dividing the logical address into multiple parts to manage the page table as a tree structure (typically 4-level or 5-level paging in modern systems). This method offers two key advantages:

\begin{itemize}
	\item \textbf{Memory Efficiency:} The system does not need to store the entire page table structure in RAM. Due to the sparse nature of address spaces, Inner Tables are only initialized for the virtual memory regions that the process actually uses.
	\item \textbf{Non-contiguous Allocation:} Only the highest-level table (Outer Page Table)—which is usually very small—needs to be contiguous. The Inner Tables can be scattered anywhere in physical memory, reducing the burden on the OS's memory management.
\end{itemize}

%\subsection*{Disadvantages}
\noindent However, this method comes with significant disadvantages regarding performance and complexity:

\begin{itemize}
	\item \textbf{Access Latency:} The number of memory accesses increases with the number of paging levels. For instance, with 5-level paging, the CPU requires 6 memory accesses to retrieve a single byte of data (5 for the page table walk and 1 for the actual data), significantly slowing down processing speed.
	\item \textbf{Design Complexity:} Both the hardware (specifically the MMU) and the Operating System must be designed with greater complexity to handle the page walk mechanism accurately.
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Hierarchical Paging}
	\caption{5-Level Paging Scheme (Address Translation for 57-bit Virtual Address)}
\end{figure}

%	This method divides the logical address into multiple parts to manage the page table as a tree structure (4-level or 5-level paging in modern systems).


%\noindent \textbf{Advantages}
%	\begin{itemize}
	%		\item \textbf{Memory Efficiency for Sparse Addresses:} The system does not need to allocate memory for the entire page table at once. It solves the problem where a single-level table would require an enormous amount of contiguous memory. It allows dividing the page table into smaller units.
	
	%	\end{itemize}

%	\noindent \textbf{Disadvantages}
%	\begin{itemize}
	%		 \item \textbf{Memory Access Latency:} As the address space expands (e.g., to 64-bit), the number of levels in the hierarchy increases. Without a TLB, a single memory reference might require multiple memory accesses (e.g., 5 accesses) to traverse the page table before accessing the actual data.
	%		\item \textbf{Complexity:} Managing a multi-level tree (5 levels) is more complex than a simple flat table.
	%	\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{b. Huge Pages}

As analyzed in the \textbf{Hierarchical Paging} section, while that method effectively manages memory allocation, it compromises performance because the CPU must perform multiple memory accesses to walk the page table tree. Furthermore, relying on the standard \textbf{4KB} page size in systems with massive RAM capacities results in bloated page tables and places significant pressure on the TLB.

To mitigate these overheads \textbf{directly within the hierarchical framework}, the \textbf{Huge Pages} technique is employed. Instead of rigidly dividing memory into 4KB blocks, the system aggregates them into larger chunks (typically \textbf{2MB} or \textbf{1GB}). The primary goal is to reduce the depth of the page table walk and increase the TLB reach.

Huge Pages do not eliminate the hierarchical paging structure but operate by ``short-circuiting'' the tree traversal process. Considering the Intel x86-64 4-level paging structure:
\begin{itemize}
	\item \textbf{Standard Page (4KB):} The CPU must traverse all 4 levels: PML4 $\rightarrow$ PDP $\rightarrow$ PD $\rightarrow$ PT $\rightarrow$ 4KB Page.
	\item \textbf{Huge Page (2MB):} The CPU only traverses 3 levels. At the 3rd level (Page Directory - PD), a special bit (PS bit - Page Size) is set. The entry in the PD points directly to a \textbf{2MB physical frame} instead of a Level-4 Page Table.
	\item \textbf{Huge Page (1GB):} The CPU only traverses 2 levels. At the 2nd level (Page Directory Pointer - PDP), the PS bit is set, pointing directly to a \textbf{1GB physical frame}.
\end{itemize}

%\noindent \textbf{The Role of TLB (TLB Reach)}

%\noindent The fundamental theory behind Huge Pages is the concept of %\textbf{TLB Reach}:
%\[
%\text{TLB Reach} = (\text{Number of TLB Entries}) \times (\text{Page Size})
%\]
%	The TLB is an expensive cache with very limited capacity.
%	\begin{itemize}
	%		\item \textbf{With 4KB Pages:} A TLB with 1024 entries can only cover $1024 \times 4\text{KB} = 4\text{MB}$ of memory. If an application uses 16GB of RAM, the TLB will suffer from constant \textit{TLB Thrashing}.
	%		\item \textbf{With 2MB Pages:} The same 1024-entry TLB can cover $1024 \times 2\text{MB} = 2\text{GB}$ of memory. The \textit{TLB Hit} rate increases dramatically, preventing the CPU from wasting cycles walking the page tables in RAM.
	%	\end{itemize}

\noindent \textbf{Advantages}
\begin{itemize}
	\item \textbf{Performance:} Minimizes TLB Misses. With a high TLB Hit rate, physical address translation is almost instantaneous.
	\item \textbf{Smaller Page Tables:} Since one large page replaces 512 small pages (in the case of 2MB), the number of page table entries is reduced by a factor of 512. The system does not need to allocate memory for the last-level Page Tables, saving RAM for the OS.
	\item \textbf{Fewer Memory Accesses:} The page walk process is shorter (requiring only 2--3 memory accesses instead of 4--5).
\end{itemize}

\noindent \textbf{Disadvantages}
\begin{itemize}
	\item \textbf{Internal Fragmentation:} This is the most significant drawback. If a process requires only 10KB of memory but is allocated a 2MB Huge Page, nearly 2MB is wasted.
	\item \textbf{Allocation Latency:} Finding a contiguous physical memory block of 2MB or 1GB is much more difficult than finding a 4KB block, especially when RAM is fragmented after long periods of operation.
	\item \textbf{Swapping Issues:} Swapping out a 1GB page to the disk is much more cumbersome and expensive than handling standard 4KB pages.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Huge page}
	\caption{Huge Pages}
\end{figure}



%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{c. Hashed Page Tables}

This method utilizes a \textbf{Hash Table} to manage page table entries. Instead of using linear storage or a hierarchical tree structure, the Virtual Page Number (VPN) is passed through a hash function to determine its storage location in the table. This approach is particularly effective for systems with a ``sparse address space,'' where the majority of virtual addresses are not in use. Each entry in the hash table typically serves as the head of a linked list (to handle hash collisions). Each element in the list consists of three main fields:
\begin{itemize}
	\item \textbf{Virtual Page Number (VPN):} Used to identify the page.
	\item \textbf{Physical Frame Number (PFN):} The corresponding mapped physical frame value.
	\item \textbf{Pointer:} A reference to the next element in the linked list.
\end{itemize}

%	\noindent The address translation process proceeds as follows:
%	\begin{itemize}
	%		\item The CPU extracts the virtual page number ($p$) from the logical address and applies a hash function to generate an index $h(p)$.
	%		\item The system accesses the corresponding entry in the hash table and begins traversing the linked list at that location. For each element, the system compares $p$ with the stored VPN field:
	%		\begin{itemize}
		%			\item \textbf{If there is a match:} The system retrieves the corresponding PFN value and combines it with the offset to form the actual physical address.
		%			\item \textbf{If there is no match:} The system proceeds to the next element in the list to continue the search.
		%		\end{itemize}
	%	\end{itemize}

\noindent \textbf{Advantages}
\begin{itemize}
	\item \textbf{Optimal Memory Efficiency:} The size of the hashed page table is proportional to the number of \textit{physical pages actually in use}, rather than the size of the entire virtual address space. This results in significant memory savings in massive 64-bit systems.
	\item \textbf{Flexible Structure:} Since it does not require storing information for unused memory regions, this method eliminates memory waste associated with page tables for unallocated address ranges.
\end{itemize}

\noindent \textbf{Disadvantages}
\begin{itemize}
	\item \textbf{Variable Memory Access Cost:} Performance depends heavily on the quality of the hash function. High collision rates result in longer linked lists, forcing the CPU to perform multiple memory accesses to traverse the list, thereby reducing processing speed.
	\item \textbf{Hardware Design Complexity:} Traversing a linked list is more difficult to implement in hardware (MMU) compared to walking a tree structure in hierarchical paging.
	\item \textbf{Poor Locality of Reference:} Due to the random nature of hashing, pages that are contiguous in virtual memory may be scattered across the hash table, reducing the efficiency of the CPU Cache.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Hashed Page Tables}
	\caption{Hashed Page Tables}
\end{figure}

\noindent \textbf{d. Clustered Page Tables}

Clustered Page Tables are an optimized variation of the \textbf{Hashed Page Tables} method. As previously discussed, traditional hashed page tables store individual $\langle \text{VPN, PFN} \rangle$ mappings. This can lead to significant memory overhead for pointers and result in long linked list chains if hash collisions occur frequently.

To address this, \textbf{Clustered Page Tables} modify the storage unit: instead of each entry containing information for a single page, it stores mapping information for a group of \textbf{contiguous virtual pages} (e.g., 16 consecutive pages).

\noindent \textbf{Entry Structure:} Each entry in the hash table's linked list typically consists of:
\begin{itemize}
	\item \textbf{Virtual Page Number (VPN):} The identifier tag for the first page in the cluster (Base VPN).
	\item \textbf{An Array of PFNs:} Stores physical frame numbers for multiple consecutive pages (e.g., $PFN_0, PFN_1, \dots, PFN_{15}$).
	\item \textbf{Pointer:} Points to the next entry in case of a hash collision.
\end{itemize}

\noindent \textbf{Mechanism:} When the CPU needs to translate a virtual page $p$:
\begin{itemize}
	\item The system hashes $p$ to locate the bucket in the table.
	\item It searches the linked list for an entry where the \textbf{Base VPN} matches $p$.
	\item Instead of retrieving just one address, the system can access the address for $p$ and its neighbors ($p+1, p+2\dots$) located within the same cluster.
\end{itemize}

\noindent \textbf{Advantages}
\begin{itemize}
	\item \textbf{Reduced Pointer Overhead:} Since one entry stores multiple pages (e.g., 16 pages), the number of nodes in the linked list is reduced by a factor of 16, saving memory space used for `next` pointers.
	\item \textbf{Leveraging Spatial Locality:} Programs often access memory regions that are close to each other. Grouping these pages into a single entry improves performance when processing contiguous data blocks.
	\item \textbf{Sparse Address Space Support:} Retains the benefit of Hashed Page Tables by not consuming memory for unused virtual address regions.
\end{itemize}

\noindent \textbf{Disadvantages}
\begin{itemize}
	\item \textbf{Implementation Complexity:} Managing clusters requires more complex software/hardware logic compared to simple 1-to-1 mapping.
	\item \textbf{Internal Fragmentation within Entry:} If a cluster is designed to hold 16 pages, but a process only uses 1 page within that cluster, the remaining 15 PFN storage slots in that entry remain empty, resulting in wasted page table memory.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{Clustered Page Tables}
	\caption{Clustered Page Tables}
\end{figure}
%	This method uses a hash function on the virtual page number to locate the physical frame.


%	\noindent \textbf{Advantages}
%	\begin{itemize}
	%		 \item \textbf{Suitability for Large Address Spaces:} This structure is common in address spaces larger than 32 bits.
	%		 \item \textbf{Handling Sparse Memory:} It is especially useful for sparse address spaces where memory references are non-contiguous and scattered.
	%	\end{itemize}

%	\noindent \textbf{Disadvantages}
%	\begin{itemize}
	%		 \item \textbf{Collision Handling:} Multiple virtual addresses may hash to the same location, requiring a chain of elements to be searched to find a match.
	%		\item \textbf{Implementation Complexity:} While efficient for size, the overhead of hashing and traversing linked lists (chains) can impact performance compared to direct indexing.
	%	\end{itemize}


\noindent \textbf{e. Inverted Page Tables}

In both Hierarchical Paging and Hashed Page Tables methods, each process maintains its own separate page table. Each table contains an entry for every virtual page the process is using. This results in the system managing millions of entries, consuming a significant amount of physical memory just to store page table structures to track memory usage.

To solve this problem, the \textbf{Inverted Page Table} method was developed. Instead of indexing based on the virtual address space of each process, this method indexes based on the frames of the actual \textbf{physical memory}.

\noindent \textbf{Structure and Mechanism}
\begin{itemize}
	\item \textbf{Global Nature:} There is only one Global Page Table for the entire system, regardless of the number of running processes.
	\item \textbf{Fixed Size:} The number of entries in this table corresponds exactly to the number of \textbf{Physical Frames} in RAM. For example, if RAM has 1 million frames, the table will have 1 million entries.
	\item \textbf{Entry Structure:} The $i$-th entry in the table contains information about the virtual page stored in the $i$-th physical frame. Each entry typically consists of:
	\begin{enumerate}
		\item \textbf{Virtual Page Address:} To identify the logical page.
		\item \textbf{Process ID (PID):} Information about the process that owns the page (essential because different processes may generate identical virtual addresses).
	\end{enumerate}
\end{itemize}

\noindent \textbf{Advantages}
\begin{itemize}
	\item \textbf{Maximum Memory Efficiency:} This is the most significant advantage. The size of the page table is fixed and depends only on the size of physical RAM, making it independent of the number of processes or the size of the virtual address space.
\end{itemize}

\noindent \textbf{Disadvantages}
\begin{itemize}
	\item \textbf{High Lookup Latency:} Since the table is sorted by physical address (while the CPU queries based on virtual address), the system cannot use direct indexing ($O(1)$) as in traditional methods. Instead, it must perform a search operation. Although often assisted by a Hash Table, handling collisions and traversing the table is inherently slower than direct array access.
	\item \textbf{Difficulty in Shared Memory:} This is the major drawback. In traditional paging, memory sharing is achieved by having entries in different process page tables point to the same physical frame. However, in an Inverted Page Table, each physical frame corresponds to exactly \textbf{one single entry}. Therefore, it can only store one $\langle \text{PID, VPN} \rangle$ pair, making the implementation of shared memory complex and often requiring auxiliary data structures.
\end{itemize}
%	 Instead of one page table per process, there is one entry for each real page (frame) of physical memory.


%	\noindent \textbf{Advantages}
%	\begin{itemize}
	%		 \item \textbf{Memory Savings:} This approach significantly decreases the amount of memory needed to store each page table, as the table size is proportional to physical memory, not the virtual address space.
	%	\end{itemize}

%	\noindent \textbf{Disadvantages}
%	\begin{itemize}
	%		 \item \textbf{Increased Lookup Time:} It increases the time needed to search the table when a page reference occurs, often requiring a hash table to limit the search.
	%		 \item \textbf{Difficulty with Shared Memory:} Implementing shared memory is difficult because there is typically only one mapping of a virtual address to a shared physical address.
	%	\end{itemize}


\paragraph{f. Reasons for Choosing Hierarchical Page Tables}

In this simple operating system simulation project, we selected Hierarchical Page Tables (5-level Paging Scheme) for the following three reasons:

\begin{itemize}
	\item \textbf{Dynamic Allocation:} \\
	The hierarchical structure enables ``on-demand'' memory allocation. There is no need to initialize the entire massive data structure upfront. When a process requires access to a memory region, we simply invoke \texttt{alloc\_aligned\_table()} for that specific branch. This approach effectively prevents the wastage of management memory within a 64-bit environment.
	
	\item \textbf{Simplified Implementation of Sharing and Protection:} \\
	The Address-Translation model allows for the assignment of access permissions (Read/Write/Execute) at each level of the tree (from top-level directories down to individual pages). This facilitates the implementation of Memory Protection, allowing for easy setup and granular control over specific memory regions.
	
	\item \textbf{Ease of Implementation:} \\
	Compared to Inverted Page Tables or Hashed Page Tables, the hierarchical structure is easier to implement within a C-based simulation environment. We primarily rely on bitwise shifts and array indexing to decompose virtual addresses into their respective indices .
\end{itemize}
%	\begin{figure}[H]
	%		\centering
	%		\includegraphics[width=1\textwidth]{Hierarchical Page Tables}
	%		\caption{5-Level Paging Scheme (Address Translation for 57-bit Virtual Address)}
	%	\end{figure}
	
\pagebreak

\section{Implementation}

\subsection{Scheduler}

\subsubsection{Queue Operations (Implementation of Round-Robin)}

To support the Round-Robin scheduling within each priority level, the queue operations in \texttt{queue.c} must strictly enforce First-In-First-Out (FIFO) behavior. I have implemented three core functions as follows:

\paragraph{a. Enqueue Operation (Arrival/Preemption)}
The \texttt{enqueue} function handles the arrival of a new process or the return of a preempted process.
\begin{itemize}
    \item \textbf{Mechanism:} It places the process at index \texttt{q->size}. This is critical for Round-Robin: a process finishing its time slice is moved to the back of the line. We check for buffer overflow (\texttt{MAX\_QUEUE\_SIZE}) before assignment to ensure memory safety.
\end{itemize}

\begin{lstlisting}[language=C, caption=Enqueue Implementation]
void enqueue(struct queue_t *q, struct pcb_t *proc) {
    /* TODO: put a new process to queue [q] */
    if (q->size < MAX_QUEUE_SIZE) {
        q->proc[q->size] = proc;
        q->size++;
    }
}
\end{lstlisting}

\paragraph{b. Dequeue Operation (Dispatching)}
The \texttt{dequeue} function selects the next process for execution.
\begin{itemize}
    \item \textbf{Mechanism:} It selects the process at index 0 (the Head), which corresponds to the process that has been waiting the longest. Since we use a static array, removing index 0 leaves a "hole". The function includes a loop to shift all subsequent elements ($i+1$) to position $i$. This ensures the array remains contiguous and the next candidate is always at index 0. 
\end{itemize}

\begin{lstlisting}[language=C, caption=Dequeue Implementation]
struct pcb_t *dequeue(struct queue_t *q) {
    /* TODO: return a pcb whose prioprity is the highest
     * in the queue [q] and remember to remove it from q */
        if (q == NULL || q->size == 0) {
        return NULL;
    }
    struct pcb_t *first_proc = q->proc[0];

    int i;
    for (i = 0; i < q->size - 1; i++) {
        q->proc[i] = q->proc[i + 1];
    }
    q->size--;
    return first_proc;
}
\end{lstlisting}


\subsubsection{MLQ Scheduler Implementation (sched.c)}

The \texttt{sched.c} module implements the core logic of the Multi-Level Queue scheduler. This implementation is designed to handle process retrieval and placement within a Symmetric Multiprocessing (SMP) environment, strictly following the Slot-based policy defined in the assignment specifications.

\paragraph{a. Scheduler Initialization (\texttt{init\_scheduler})}
This function sets up the system state before any process execution begins.

\begin{itemize}
    \item \textbf{Slot Pre-calculation:} Instead of calculating the slot quota at runtime, we initialize the \texttt{slot} array immediately using the formula $slot[i] = MAX\_PRIO - i$. 
    \item \textbf{Reasoning:} This optimization reduces CPU overhead during the critical context-switching phase. By pre-filling the quotas, the scheduler simply needs to decrement or reset values during execution.
    \item \textbf{Mutex Initialization:} The \texttt{pthread\_mutex\_init} call prepares the lock required for thread safety in the SMP architecture.
\end{itemize}

\begin{lstlisting}[caption={Initialization Logic}]
void init_scheduler(void) {
    int i;
    for (i = 0; i < MAX_PRIO; i++) {
        mlq_ready_queue[i].size = 0;
        slot[i] = MAX_PRIO - i; // Pre-calculate quotas
    }
    // ...
    pthread_mutex_init(&queue_lock, NULL);
}
\end{lstlisting}

\paragraph{b. Process Retrieval Logic (\texttt{get\_mlq\_proc}):}

The \texttt{get\_mlq\_proc} function serves as the core of the scheduler, responsible for selecting the next process for CPU allocation from the set of priority queues. The algorithm is designed based on the Multi-Level Queue (MLQ) model combined with a \textit{Slot-based Allocation} mechanism to address the issue of resource starvation.

\begin{enumerate}
    \item \textbf{General Operating Principle}
    
    The function iterates sequentially through the queues from the highest priority (\texttt{prio = 0}) to the lowest (\texttt{prio = MAX\_PRIO - 1}). The decision to select a process is based on two prerequisites:
    \begin{itemize}
        \item \textbf{Readiness:} The queue at that priority level must have waiting processes (\texttt{!empty}).
        \item \textbf{Resource Quota (Slot):} The queue must have remaining CPU usage quota (\texttt{slot[i] > 0}).
    \end{itemize}

    \item \textbf{Algorithm Flow}
    
    The process selection procedure occurs through the following steps:
    
    \begin{itemize}
        \item \textbf{Step 1: Critical Section Protection.} Since the system simulates a multi-processor environment, the function begins by locking the Mutex (\texttt{pthread\_mutex\_lock(\&queue\_lock)}). This ensures the data integrity of the \texttt{mlq\_ready\_queue} when multiple CPUs access it simultaneously.
        
        \item \textbf{Step 2: Priority Traversal.} A loop iterates from \texttt{i = 0} (highest priority) to \texttt{MAX\_PRIO}. At each priority level \texttt{i}:
        \begin{itemize}
            \item \textit{Check Empty:} If \texttt{mlq\_ready\_queue[i]} is empty, the system skips it and checks the next priority level.
            \item \textit{Check Slot (Core Logic):}
            \begin{itemize}
                \item \textbf{Case A (Slot Remaining - \texttt{slot[i] > 0}):} The queue is allowed to run. The system retrieves the first process (dequeue) and decrements the remaining slots by 1 (\texttt{slot[i]-{}-}). \textbf{Decision:} Break the loop immediately to return this process to the CPU.
                \item \textbf{Case B (Slot Exhausted - \texttt{slot[i] == 0}):} The queue has used up its quota for the current cycle. The system performs a Slot Refill using the formula: \texttt{slot[i] = MAX\_PRIO - i}. \textbf{Decision:} Crucially, the algorithm DOES NOT select a process from this queue but continues the loop (\texttt{continue}) to check the next lower priority queue.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Step 3: System State Update.} If a process is found (\texttt{proc != NULL}), it is added to the running list (\texttt{running\_list}) for tracking.
        
        \item \textbf{Step 4: Finish.} Unlock the Mutex (\texttt{pthread\_mutex\_unlock}) and return the \texttt{pcb\_t} pointer of the selected process (or \texttt{NULL} if no viable process exists).
    \end{itemize}

    \item \textbf{Slot-based Mechanism Analysis}
    
    
    This implements a mechanism of \textbf{"Conditional Yielding"}:
    \begin{itemize}
        \item \textbf{Purpose:} To prevent high-priority processes (e.g., Priority 0) from monopolizing the CPU indefinitely.
        \item \textbf{Operation:} When a high-priority queue consumes its allocated slots (e.g., 140 slots for Prio 0), it is forced to reset its slots and "yield" the checking turn to lower-priority queues within the current function call. This ensures relative fairness, allowing lower-priority processes (such as Prio 139) a chance to execute (CPU time) even under heavy load conditions.
    \end{itemize}

    \item \textbf{Complexity Analysis}
    
    In the worst-case scenario, the algorithm must traverse through all \texttt{MAX\_PRIO} queues.
    \\
    \textbf{Complexity:} $O(K + N)$
    \begin{itemize}
        \item $K$: The number of priority levels (\texttt{MAX\_PRIO}).
        \item $N$: The cost of the \texttt{dequeue} operation.
    \end{itemize}
\end{enumerate}

\begin{lstlisting}[caption={Slot-based Decision Logic}]
struct pcb_t * get_mlq_proc(void) {
    struct pcb_t * proc = NULL;

    pthread_mutex_lock(&queue_lock);
    
    for (int i = 0; i < MAX_PRIO; i++) {
        if (!empty(&mlq_ready_queue[i])) {
            if (slot[i] > 0) {
                proc = dequeue(&mlq_ready_queue[i]);
                slot[i]--;
                break; 
            } else {
                slot[i] = MAX_PRIO - i;
            }
        }
    }

    if (proc != NULL)
        enqueue(&running_list, proc);
    
    pthread_mutex_unlock(&queue_lock);
    
    return proc;    
}
}
\end{lstlisting}

\paragraph{c. Process Placement (\texttt{put\_mlq\_proc} / \texttt{add\_mlq\_proc})}
These functions handle putting a process back into the ready queue (e.g., after a time slice expires or upon creation).

\begin{itemize}
    \item \textbf{Mechanism:} They map the process's priority (\texttt{proc->prio}) to the correct index in the \texttt{mlq\_ready\_queue} array and invoke the FIFO \texttt{enqueue} operation.
    \item \textbf{Synchronization:} These operations are wrapped in mutex locks to ensure the queue structure is not corrupted by concurrent access from the loader or other CPUs.
\end{itemize}

\paragraph{d. Synchronization Mechanism (Mutex Protection)}
A critical aspect of this implementation is the handling of Shared Resources in a multi-core environment.

In this simulation, the \texttt{mlq\_ready\_queue} is a \textbf{Global Shared Resource}. 
\begin{itemize}
    \item \textit{Scenario without Mutex:} CPU 0 checks queue 0 and sees 1 process. Simultaneously, CPU 1 checks queue 0 and sees the same process. Both CPUs try to \texttt{dequeue} the same pointer. This creates a \textbf{Race Condition}, leading to memory corruption or one process being executed twice.
\end{itemize}

We use \texttt{pthread\_mutex\_lock(\&queue\_lock)} to define a \textbf{Critical Section}.
\begin{itemize}
    \item \textbf{Scope:} The lock covers the entire duration of reading the state (checking \texttt{empty} and \texttt{slot}) and modifying the state (\texttt{dequeue}, \texttt{enqueue}, \texttt{$slot--$}).
    \item \textbf{Atomicity:} This ensures that the complex operation of "Check Slot $\rightarrow$ Dequeue $\rightarrow$ Decrement Slot" appears as a single atomic instruction to other CPUs. No other thread can interrupt or modify the queue during this sequence.
\end{itemize}

\begin{lstlisting}[caption={Critical Section Protection}]
pthread_mutex_lock(&queue_lock); // BEGIN Critical Section
// ... 
// Safe access to shared mlq_ready_queue and slot arrays
// ...
pthread_mutex_unlock(&queue_lock); // END Critical Section
\end{lstlisting}

\subsubsection{Scheduler Algorithm Conclusion}

This section describes the implementation of a \textbf{Slot-based Multi-Level Queue (MLQ)} scheduler for an SMP environment. By integrating a quota mechanism ($slot = MAX\_PRIO - prio$) into the priority traversal logic, the design \textbf{aims to balance} two conflicting objectives: prioritizing critical tasks while mitigating the starvation of lower-priority processes. Additionally, the application of \textbf{Mutex locks} \textbf{is designed to maintain} thread safety, \textbf{addressing} potential race conditions as multiple CPUs concurrently access the shared ready queue structure.


\subsection{Memory Management}

\subsubsection{Memory management}

\paragraph{a. Overview}

This module implements logical memory management per process in a simplified way. Each process owns an \texttt{mm\_struct} that contains:
\begin{itemize}
    \item One user \texttt{vm\_area\_struct} (\texttt{vmaid = 0}) acting as the container for the whole user address space,
    \item A fixed-size symbol region table \texttt{symrgtbl[]} that stores allocated regions by \texttt{rgid},
    \item And the root pointers to the multi-level page tables handled by \texttt{mm64.c}.
\end{itemize}

Instead of multiple VMAs for text/data/heap/stack, our design keeps a single user VMA and represents logical ``segments'' as virtual memory regions (\texttt{vm\_rg\_struct}, VMR) inside that VMA. Each VMR is a contiguous range \texttt{[rg\_start, rg\_end)}, recorded in \texttt{symrgtbl[rgid]}, while free gaps are chained in \texttt{vm\_freerg\_list} for reuse.

The core design principles are:
\begin{itemize}
    \item \textbf{VMA / region management:} All user regions live inside one user \texttt{vm\_area\_struct}; logical segments are \texttt{vm\_rg\_struct} descriptors, and free holes are kept in \texttt{vm\_freerg\_list}.
    
    \item \textbf{User / Kernel separation:} User code never touches kernel structures (\texttt{pcb\_t}, \texttt{mm\_struct}); it uses system calls and PID, and the kernel side looks up the correct PCB/MM before operating.
    
    \item \textbf{Allocation strategy:} Always try to reuse holes from \texttt{vm\_freerg\_list} first; if none fits, expand the VMA (\texttt{sbrk} / \texttt{vm\_end}) via the \texttt{SYSMEM\_INC\_OP} syscall, then let the paging module map the new pages.
\end{itemize}

\paragraph{b. Implementation Details}
The following code snippets illustrate the core logic handling VMA organization and the secure User-Kernel interface.

\textbf{1. VMA Data Structure (\texttt{os\_mm.h})} \\
The \texttt{vm\_area\_struct} represents a contiguous range of logical addresses $[vm\_start, vm\_end]$.
\begin{lstlisting}[language=C, caption={Virtual Memory Area Structure}]
struct vm_area_struct {
    unsigned long vm_id;    // Region ID (e.g., DATA, HEAP)
    unsigned long vm_start; // Logical Start Address
    unsigned long vm_end;   // Logical End Address
    struct vm_rg_struct *vm_freerg_list; // List of free holes for reuse
    struct vm_area_struct *vm_next;      // Pointer to next VMA
};
\end{lstlisting}

\textbf{2. Secure User/Kernel Interface (\texttt{libstd.c})} \\
This wrapper demonstrates the protection mechanism. The user space only passes value arguments and the \texttt{pid}, never the raw PCB pointer, ensuring isolation.
\begin{lstlisting}[language=C, caption={User Space System Call Wrapper}]
int libsyscall(struct pcb_t *caller, uint32_t syscall_idx, 
               arg_t a1, arg_t a2, arg_t a3)
{
    struct sc_regs regs;
    regs.a1 = a1; regs.a2 = a2; regs.a3 = a3;

    /* CRITICAL: Only caller->pid is passed to the kernel.
       The Kernel must resolve the PCB internally. */
    return syscall(caller->krnl, caller->pid, syscall_idx, &regs);
}
\end{lstlisting}

\textbf{3. Memory Allocation Algorithm - \texttt{\_\_alloc} (\texttt{libmem.c})} \\
This function executes in \textbf{Kernel Mode} after the PID has been validated. It implements the logic to search for available memory space within the VMA.
\begin{lstlisting}[language=C, caption={Kernel Space Allocation Routine}]
int __alloc(struct pcb_t *caller, int vmaid, int rgid, addr_t size, addr_t *alloc_addr)
{
    pthread_mutex_lock(&mmvm_lock);
    struct vm_rg_struct rgnode;

    // Check if we can reuse a free region
    if (get_free_vmrg_area(caller, vmaid, size, &rgnode) == 0)
    {
        caller->mm->symrgtbl[rgid].rg_start = rgnode.rg_start;
        caller->mm->symrgtbl[rgid].rg_end = rgnode.rg_end;
        *alloc_addr = rgnode.rg_start;
        pthread_mutex_unlock(&mmvm_lock);
        return 0;
    }

    /* If get_free_vmrg_area FAILED, we must expand the heap */
    struct vm_area_struct *cur_vma = get_vma_by_num(caller->mm, vmaid);
    if (!cur_vma)
    {
        pthread_mutex_unlock(&mmvm_lock);
        return -1;
    }

    addr_t inc_sz = PAGING_PAGE_ALIGNSZ(size);
    // Usually aligning to page size is good practice, or use exact size if your inc_vma handles it.
    // The provided skeleton had some ifdef logic, let's stick to standard alignment.

    addr_t old_sbrk = cur_vma->sbrk;

    /* SYSCALL to increase limit */
    struct sc_regs regs;
    regs.a1 = SYSMEM_INC_OP;
    regs.a2 = vmaid;
    regs.a3 = inc_sz; // Request expansion

    // Note: In real syscall, we don't pass PCB, but here we simulate it via wrapper
    // The wrapper 'syscall' takes (krnl, pid, nr, regs)
    if (syscall(caller->krnl, caller->pid, 17, &regs) < 0)
    {
        pthread_mutex_unlock(&mmvm_lock);
        return -1; // Failed to expand
    }

    /* Successful increase limit */
    caller->mm->symrgtbl[rgid].rg_start = old_sbrk;
    caller->mm->symrgtbl[rgid].rg_end = old_sbrk + size;
    *alloc_addr = old_sbrk;

    // The remaining space (inc_sz - size) should technically be added to free list
    // to avoid internal fragmentation, but for this simple assignment we might skip it
    // or add logic:
    if (inc_sz > size)
    {
        struct vm_rg_struct *fragment = malloc(sizeof(struct vm_rg_struct));
        fragment->rg_start = old_sbrk + size;
        fragment->rg_end = old_sbrk + inc_sz;
        fragment->rg_next = NULL;
        enlist_vm_freerg_list(caller->mm, fragment);
    }

    pthread_mutex_unlock(&mmvm_lock);
    return 0;
}
\end{lstlisting}

\textbf{4. Memory Deallocation Algorithm - \texttt{enlist\_vm\_freerg\_list} (\texttt{libmem.c})} \\
This function inserts a recently freed memory region back into the reuse list (free list), marking it available for future allocations.
\begin{lstlisting}[language=C, caption={Deallocation Logic}]
int enlist_vm_freerg_list(struct mm_struct *mm, struct vm_rg_struct *rg_elmt)
{
    struct vm_rg_struct *rg_node = mm->mmap->vm_freerg_list;

    if (rg_elmt->rg_start >= rg_elmt->rg_end)
        return -1;

    if (rg_node != NULL)
        rg_elmt->rg_next = rg_node;

    /* Enlist the new region */
    mm->mmap->vm_freerg_list = rg_elmt;

    return 0;
}
\end{lstlisting}

\paragraph{c. Algorithm Description}
\textbf{Step 1: User-Kernel Transition (Protection Mechanism)}
To ensure memory safety (referencing \textit{Silberschatz Chapter 2}), the allocation process begins with a System Call:
\begin{itemize}
    \item \textbf{User Request:} The user program calls a library function (e.g., \texttt{malloc}), which triggers the wrapper \texttt{libsyscall}.
   \item \textbf{Trap to OS:} \texttt{libsyscall} passes the Process ID (\texttt{PID}) and an operation code (such as \texttt{SYSMEM\_INC\_OP} in our allocation path, or \texttt{SYSMEM\_MAP\_OP} in other mapping scenarios) to the Kernel via a software interrupt/trap.
    \item \textbf{Authentication:} Upon entering Kernel Mode, the kernel uses the provided \texttt{PID} to traverse the process table and retrieve the correct \texttt{pcb\_t} pointer. This prevents any user process from manipulating the memory of another process.
\end{itemize}

\textbf{Step 2: VMA Management (\texttt{get\_vma\_by\_num})} \\
Once in Kernel Mode, the system must identify which memory region the process is requesting.
\begin{itemize}
    \item \textbf{Activity:} The function \texttt{get\_vma\_by\_num} traverses the linked list \texttt{mm->mmap} to find the VMA whose \texttt{vm\_id} equals the requested \texttt{vmaid}.
    
    \item \textbf{Purpose:} In our implementation, there is exactly one user VMA per process (\texttt{vmaid = 0}), which acts as the container for all user regions. The logical separation between different user ``segments'' (variables/objects) is handled by \texttt{vm\_rg\_struct} entries stored in \texttt{mm->symrgtbl[]}, rather than by multiple VMAs.
\end{itemize}

\textbf{Step 3: Memory Allocation (\texttt{\_\_alloc})} \\
The allocation logic prioritizes filling "holes" in the memory space before expanding it, ensuring resource efficiency.
\begin{enumerate}
    \item \textbf{Reuse Strategy:} The system inspects the \texttt{vm\_freerg\_list} of the current VMA. This list contains memory regions that were previously freed. The algorithm applies a \textbf{First-Fit} strategy: it traverses the list, and if a node satisfying $rg\_end - rg\_start \ge size$ is found, the system either splits the node or claims the entire node for the new allocation.
    \item \textbf{Expansion Strategy:} If the free list is empty or contains no suitable regions, the system performs a linear expansion. The new allocation address starts at the current \texttt{vm\_end}, and the boundary is updated: $vm\_end = vm\_end + size$. This behavior is analogous to the Unix \texttt{sbrk()} system call.
    \item \textbf{Registration:} Finally, the system updates the \texttt{symrgtbl} (Symbol Region Table) with the \texttt{rgid}. This maps a User-managed Region ID to the actual Kernel-managed logical address.
\end{enumerate}

\textbf{Step 4: Memory Deallocation (\texttt{enlist\_vm\_freerg\_list})} \\
When a process frees memory, the region is not immediately physically erased but transitioned to an "Available" state.
\begin{itemize}
    \item \textbf{Logic:} The function receives a memory region structure (\texttt{rg\_elmt}).
    \item \textbf{Operation:} It inserts this node into the head of the singly linked list \texttt{vm\_freerg\_list}.
    \item \textbf{Significance:} This turns used memory into a new "hole". The next allocation request (via \texttt{\_\_alloc}) will scan and potentially reuse this region, thereby minimizing external fragmentation of the logical address space.
\end{itemize}

\textbf{Conclusion:} \\
The Memory Management module at the Logical Level \textbf{implements} a memory management mechanism \textbf{predicated on} strict User/Kernel separation and an allocation strategy \textbf{utilizing} Free List reuse. This architecture \textbf{functions as} the initial abstraction layer, establishing the logical address space before these addresses are translated into physical frames by the 5-Level Paging mechanism detailed in the subsequent section.

\subsubsection{Multi-level paging}

\paragraph{a. Implementation Details}
The following core components constitute the paging mechanism.

\textbf{1. Bit Masking and Address Decoding (\texttt{mm64.h})}
The system uses macros to "peel off" each layer of the virtual address.
\begin{lstlisting}[language=C, caption={Bit Manipulation Macros in mm64.h}]
/* PGD Index: Extract 9 bits from position 48 to 56 */
#define PAGING64_ADDR_PGD_MASK   GENMASK64(56, 48)
#define PAGING64_ADDR_PGD_LOBIT  48
#define PAGING64_ADDR_PGD(addr)  ((addr & PAGING64_ADDR_PGD_MASK) >> PAGING64_ADDR_PGD_LOBIT)

/* P4D -> PUD -> PMD: Shift the bit window downwards */
#define PAGING64_ADDR_P4D(addr)  ((addr & GENMASK64(47, 39)) >> 39)
#define PAGING64_ADDR_PUD(addr)  ((addr & GENMASK64(38, 30)) >> 30)
#define PAGING64_ADDR_PMD(addr)  ((addr & GENMASK64(29, 21)) >> 21)

/* PT Index (Leaf Level): Bits 20-12 */
#define PAGING64_ADDR_PT_MASK    GENMASK64(20, 12)
#define PAGING64_ADDR_PT(addr)   ((addr & PAGING64_ADDR_PT_MASK) >> 12)

/* Offset: Last 12 bits */
#define PAGING64_ADDR_OFFST(addr) (addr & GENMASK64(11, 0))
\end{lstlisting}

\textbf{Page Table Entry - PTE (\texttt{mm64.h} \& \texttt{mm64.c})}
The PTE stores the page state. Manipulating the PTE requires bitwise operations ($|$, $\&$, $\sim$).
\begin{lstlisting}[language=C, caption={PTE Definition and Initialization}]
/* mm64.h - PTE Bit Definitions */
#define PAGING_PTE_PRESENT_MASK BIT_ULL(0)  // Bit 0: Page is in RAM
#define PAGING_PTE_DIRTY_MASK   BIT_ULL(6)  // Bit 6: Page was written to
#define PAGING_PTE_SWAPPED_MASK BIT_ULL(9)  // Bit 9: Page is swapped out
#define PAGING_PTE_FPN_MASK     GENMASK64(51, 12) // Bits 12-51: Frame Number

/* mm64.c - PTE Initialization Function */
int init_pte(addr_t *pte, int pre, addr_t fpn, int drt, int swp, int swptyp, addr_t swpoff) {
    if (pre != 0) { // If page exists (Present)
        if (swp == 0) { // Case 1: Page is in RAM
            SETBIT(*pte, PAGING_PTE_PRESENT_MASK);
            CLRBIT(*pte, PAGING_PTE_SWAPPED_MASK);
            /* Write FPN to bits 12-51 */
            SETVAL(*pte, fpn, PAGING_PTE_FPN_MASK, PAGING_PTE_FPN_LOBIT);
        } else { // Case 2: Page is Swapped
            SETBIT(*pte, PAGING_PTE_SWAPPED_MASK);
            CLRBIT(*pte, PAGING_PTE_PRESENT_MASK);
            /* Write Swap Offset */
            SETVAL(*pte, swpoff, PAGING_PTE_SWPOFF_MASK, PAGING_PTE_SWPOFF_LOBIT);
        }
    }
    return 0;
}
\end{lstlisting}

\textbf{3. Page Walk Mechanism (\texttt{mm64.c})}
The \texttt{ get\_page\_table\_entry} function is the core of translation, traversing 5 table levels.
\begin{lstlisting}[language=C, caption={Page Walk Logic (Simplified)}]
uint64_t *get_page_table_entry(struct mm_struct *mm, addr_t addr, int alloc)
{
    // Get Indices
    int pgd_idx = PAGING64_ADDR_PGD(addr);
    int p4d_idx = PAGING64_ADDR_P4D(addr);
    int pud_idx = PAGING64_ADDR_PUD(addr);
    int pmd_idx = PAGING64_ADDR_PMD(addr);
    int pt_idx = PAGING64_ADDR_PT(addr);

    // Check Root (PGD)
    if (mm->pgd == NULL)
    {
        if (!alloc)
            return NULL;
        mm->pgd = alloc_aligned_table();

        if (!mm->pgd)
            return NULL;
        memset(mm->pgd, 0, sizeof(struct pgd_t));
    }

    // Walk PGD -> P4D
    struct p4d_t *p4d_table = get_next_level((uint64_t *)&mm->pgd->entries[pgd_idx], alloc);
    if (!p4d_table)
        return NULL;

    // Walk P4D -> PUD
    struct pud_t *pud_table = get_next_level((uint64_t *)&p4d_table->entries[p4d_idx], alloc);
    if (!pud_table)
        return NULL;

    // Walk PUD -> PMD
    struct pmd_t *pmd_table = get_next_level((uint64_t *)&pud_table->entries[pud_idx], alloc);
    if (!pmd_table)
        return NULL;

    // Walk PMD -> PT
    struct pt_t *pt_table = get_next_level((uint64_t *)&pmd_table->entries[pmd_idx], alloc);
    if (!pt_table)
        return NULL;

    // Return pointer to the specific Page Table Entry (Leaf)
    return (uint64_t *)&pt_table->entries[pt_idx];
}
\end{lstlisting}

\textbf{4. Hardware Simulation (\texttt{mm-memphy.c})}
RAM is simulated not as a physical chip but as a BYTE array.
\begin{lstlisting}[language=C, caption={Physical Memory Access}]
/* mm-memphy.c */
int MEMPHY_read(struct memphy_struct *mp, addr_t addr, BYTE *value) {
    if (mp == NULL) return -1;
    /* Direct access to storage array at index addr */
    *value = mp->storage[addr];
    return 0;
}

int MEMPHY_write(struct memphy_struct *mp, addr_t addr, BYTE data) {
    if (mp == NULL) return -1;
    mp->storage[addr] = data;
    return 0;
}
\end{lstlisting}

\paragraph{b. Algorithm Description}

The system executes address translation via a strict procedure, simulating the hardware MMU (Memory Management Unit).

\textbf{Step 1: Address Decoding}
The computer views the virtual address not as a large integer, but as a collection of indices.
\begin{itemize}
    \item \textbf{Input:} 64-bit Virtual Address (VA).
    \item \textbf{Algorithm:} Use Bitwise AND (\&) with a Mask to isolate specific bits, then Bitwise Right Shift ($>>$) to normalize the value into an integer index.
    \item \textbf{Example:} To get the PGD Index, the system extracts the highest 9 bits (56-48). This value determines the position in the PGD table.
\end{itemize}

\textbf{Step 2: Page Walk (Tree Traversal)}
This is the core algorithm. Conceptually, the page table is a 5-level tree.
\begin{enumerate}
    \item \textbf{Root:} Start from \texttt{mm->pgd} (stored in the CR3 register on real CPUs).
    \item \textbf{Traversal:}
    \begin{itemize}
        \item Use PGD Index to select an entry in PGD. This entry contains the physical address of the P4D table.
        \item Use P4D Index to select an entry in P4D. This points to the PUD table.
        \item Continue similarly through PUD and PMD.
    \end{itemize}
    \item \textbf{Leaf:} At the final level (PT), use the PT Index to retrieve the \textbf{PTE}.
\end{enumerate}
\textbf{Logic Flow:} CR3 $\rightarrow$ [PGD] $\rightarrow$ [P4D] $\rightarrow$ [PUD] $\rightarrow$ [PMD] $\rightarrow$ [PT] $\rightarrow$ Frame.

\textbf{Step 3: PTE Handling \& State Check}
Once the PTE is retrieved, the system checks the status bits:
\begin{itemize}
    \item \textbf{Case A: Bit Present = 1 (Hit):} The page is in RAM. The system extracts the 40-bit FPN (Frame Page Number) from the PTE.
    \item \textbf{Case B: Bit Present = 0 \& Swapped = 1 (Page Fault):} The data is not in RAM but in Swap. The system triggers \texttt{\_\_mm\_swap\_page}: Find a free frame $\rightarrow$ Read data from Swap $\rightarrow$ Update PTE (Present=1, Swapped=0).
    \item \textbf{Case C: Bit Present = 0 \& Swapped = 0 (Invalid):} Accessing unallocated memory $\rightarrow$ Segmentation Fault.
\end{itemize}

\textbf{Step 4: Physical Address Composition}
After obtaining the frame number (FPN), the final physical address is calculated:
\[
PhysicalAddress = (FPN \times PageSize) + Offset
\]
In code, this is optimized using bitwise operations:
\begin{itemize}
    \item \texttt{FPN << 12}: Left shift 12 bits (equivalent to multiplying by 4096).
    \item \texttt{| Offset}: Bitwise OR with Offset (lower 12 bits of VA) to combine them.
\end{itemize}

\textbf{Step 5: Physical Access}
Finally, the Physical Address (PA) is passed to \texttt{mm-memphy.c}.
\begin{itemize}
    \item The module treats RAM as a massive array \texttt{storage[]}.
    \item It accesses \texttt{storage[PA]} to read or write the actual data byte.
\end{itemize}

\textbf{Conclusion}
The successful implementation of the 5-Level Paging mechanism demonstrates memory management at the finest granularity. The Page Walk algorithm, combined with precise Bitwise operations, ensures the system can translate addresses within the vast 64-bit space while transparently handling complex scenarios like Page Faults and Swapping.

\pagebreak

\section{Interprets the results of running tests}

\subsection{Scheduling}

\subsubsection{The result input file of scheduler (sched)}

The following output describes the behavior of the scheduler over multiple time slots, showing how processes are loaded, dispatched, and processed by the CPU.

\begin{lstlisting}
Time slot    0
ld_routine
        Loaded a process at input/proc/p2s, PID: 1 PRIO: 0
Time slot    1
        CPU 1: Dispatched process  1
        Loaded a process at input/proc/p1s, PID: 2 PRIO: 1
Time slot    2
        CPU 0: Dispatched process  2
Time slot    3
        Loaded a process at input/proc/p2s, PID: 3 PRIO: 1
Time slot    4
        Loaded a process at input/proc/p3s, PID: 4 PRIO: 0
Time slot    5
        CPU 1: Put process  1 to run queue
        CPU 1: Dispatched process  4
Time slot    6
        CPU 0: Put process  2 to run queue
        CPU 0: Dispatched process  1
Time slot    7
Time slot    8
Time slot    9
        CPU 1: Put process  4 to run queue
        CPU 1: Dispatched process  4
Time slot   10
        CPU 0: Put process  1 to run queue
        CPU 0: Dispatched process  1
Time slot   11
Time slot   12
Time slot   13
        CPU 1: Put process  4 to run queue
        CPU 1: Dispatched process  4
Time slot   14
        CPU 0: Processed  1 has finished
        CPU 0: Dispatched process  3
Time slot   15
Time slot   16
        CPU 1: Processed  4 has finished
        CPU 1: Dispatched process  2
Time slot   17
Time slot   18
        CPU 0: Put process  3 to run queue
        CPU 0: Dispatched process  3
Time slot   19
Time slot   20
        CPU 1: Put process  2 to run queue
        CPU 1: Dispatched process  2
Time slot   21
Time slot   22
        CPU 0: Put process  3 to run queue
        CPU 0: Dispatched process  3
        CPU 1: Processed  2 has finished
        CPU 1 stopped
Time slot   23
Time slot   24
Time slot   25
Time slot   26
        CPU 0: Processed  3 has finished
        CPU 0 stopped
\end{lstlisting}

\textbf{Explanation of Output}

The output demonstrates the behavior of the scheduler across multiple time slots and the corresponding actions performed by the CPU. Below is an analysis of key steps:

\begin{itemize}
    \item \textbf{Time Slot 0 - 4 (Loading and Initial Dispatching):}
    \begin{itemize}
        \item \textit{Loading:} Processes are loaded sequentially. Process 1 (Prio 0) loads at slot 0; Process 2 (Prio 1) at slot 1; Process 3 (Prio 1) at slot 3; and Process 4 (Prio 0) at slot 4.
        \item \textit{Dispatching:} CPU 1 picks up Process 1 at slot 1. CPU 0 picks up Process 2 at slot 2. This shows the workload being distributed across both CPUs immediately as processes arrive.
    \end{itemize}

    \item \textbf{Time Slot 5 - 6 (Preemption and Context Switching):}
    \begin{itemize}
        \item \textit{Preemption:} At slot 5, Process 1 (on CPU 1) exhausts its time slice and is put back into the run queue. CPU 1 immediately dispatches the newly loaded Process 4 (Prio 0).
        \item \textit{Process Migration:} At slot 6, Process 2 (on CPU 0) is preempted. CPU 0 then picks up Process 1 (which was previously on CPU 1). This clearly demonstrates \textit{process migration} in the SMP environment.
    \end{itemize}

    \item \textbf{Time Slot 9 - 13 (Re-dispatching):}
    \begin{itemize}
        \item \textit{Round-Robin Behavior:} Processes 4 and 1 continue to execute. At slot 9 and 13, CPU 1 puts Process 4 back to the queue but immediately re-dispatches it (likely because it has the highest priority/quota at that moment). Similarly, CPU 0 re-dispatches Process 1 at slot 10.
    \end{itemize}

    \item \textbf{Time Slot 14 - 16 (Process Completion):}
    \begin{itemize}
        \item \textit{Completion:} Process 1 finishes execution on CPU 0 at slot 14. CPU 0 then switches to Process 3 (Prio 1).
        \item \textit{Completion:} Process 4 finishes execution on CPU 1 at slot 16. CPU 1 then switches to Process 2 (Prio 1), which had been waiting since slot 6.
    \end{itemize}

    \item \textbf{Time Slot 22 - 26 (Finalization):}
    \begin{itemize}
        \item \textit{Termination:} Process 2 finishes at slot 22, and CPU 1 stops as there are no more processes for it. Process 3 continues on CPU 0 until slot 26, where it finishes, and CPU 0 finally stops.
    \end{itemize}
\end{itemize}

\subsubsection{Scheduling: Gantt chart}

\begin{figure}[h]
    \centering
    \begin{ganttchart}[
        vgrid, hgrid,
        y unit title=0.5cm,
        y unit chart=0.6cm,
        x unit=0.45cm, % Điều chỉnh độ rộng của cột
        title label anchor/.style={below=-1.6ex},
        title height=1,
        bar height=0.6,
        bar label node/.append style={align=left, text width=2cm},
        bar/.append style={fill=blue!30}, % Màu thanh bar
        include title in canvas=false
    ]{0}{26}
    
    % --- Labels (Time Slots) ---
    \gantttitle{Time Slots}{27} \\
    \gantttitlelist{0,...,26}{1} \\
    
    % --- Tasks (Processes on CPU 0) ---
    % PID 2 chạy từ slot 2 đến hết slot 5 (4 ticks)
    \ganttbar[bar/.append style={fill=green!40}]{PID2-Prio 1}{2}{5} \\
    
    % PID 1 chạy từ slot 6 đến hết slot 9 (4 ticks)
    \ganttbar[bar/.append style={fill=red!40}]{PID1-Prio 0}{6}{9} \\
    
    % PID 1 chạy tiếp từ slot 10 đến hết slot 13 (4 ticks) -> Finish tại 14
    \ganttbar[bar/.append style={fill=red!40}]{PID1-Prio 0}{10}{13} \\
    
    % PID 3 chạy từ slot 14 đến hết slot 17 (4 ticks)
    \ganttbar[bar/.append style={fill=blue!40}]{PID3-Prio 1}{14}{17} \\
    
    % PID 3 chạy từ slot 18 đến hết slot 21 (4 ticks)
    \ganttbar[bar/.append style={fill=blue!40}]{PID3-Prio 1}{18}{21} \\
    
    % PID 3 chạy từ slot 22 đến hết slot 25 (4 ticks) -> Finish tại 26
    \ganttbar[bar/.append style={fill=blue!40}]{PID3-Prio 1}{22}{25} 
    
    % Relations (Optional - visual flow)
    \ganttlink{elem0}{elem1}
    \ganttlink{elem1}{elem2}
    \ganttlink{elem2}{elem3}
    \ganttlink{elem3}{elem4}
    \ganttlink{elem4}{elem5}
    
    \end{ganttchart}
    \caption{Gantt Chart for CPU 0}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{ganttchart}[
        vgrid, hgrid,
        y unit title=0.5cm,
        y unit chart=0.6cm,
        x unit=0.45cm,
        title label anchor/.style={below=-1.6ex},
        title height=1,
        bar height=0.6,
        bar label node/.append style={align=left, text width=2cm},
        bar/.append style={fill=orange!30}, % Màu thanh bar khác cho CPU 1
        include title in canvas=false
    ]{0}{26}
    
    % --- Labels ---
    \gantttitle{Time Slots}{27} \\
    \gantttitlelist{0,...,26}{1} \\
    
    % --- Tasks (Processes on CPU 1) ---
    % PID 1 chạy từ slot 1 đến hết slot 4 (4 ticks)
    \ganttbar[bar/.append style={fill=red!40}]{PID1-Prio 0}{1}{4} \\
    
    % PID 4 chạy từ slot 5 đến hết slot 8 (4 ticks)
    \ganttbar[bar/.append style={fill=yellow!50}]{PID4-Prio 0}{5}{8} \\
    
    % PID 4 chạy từ slot 9 đến hết slot 12 (4 ticks)
    \ganttbar[bar/.append style={fill=yellow!50}]{PID4-Prio 0}{9}{12} \\
    
    % PID 4 chạy từ slot 13 đến hết slot 15 (3 ticks) -> Finish tại 16
    \ganttbar[bar/.append style={fill=yellow!50}]{PID4-Prio 0}{13}{15} \\
    
    % PID 2 chạy từ slot 16 đến hết slot 19 (4 ticks)
    \ganttbar[bar/.append style={fill=green!40}]{PID2-Prio 1}{16}{19} \\
    
    % PID 2 chạy từ slot 20 đến hết slot 21 (2 ticks) -> Finish tại 22
    \ganttbar[bar/.append style={fill=green!40}]{PID2-Prio 1}{20}{21}
    
    % Relations
    \ganttlink{elem0}{elem1}
    \ganttlink{elem1}{elem2}
    \ganttlink{elem2}{elem3}
    \ganttlink{elem3}{elem4}
    \ganttlink{elem4}{elem5}
    
    \end{ganttchart}
    \caption{Gantt Chart for CPU 1}
\end{figure}

\subsection{Memory}
\subsubsection{Result of the input file of Memory (os\_0\_mlq\_paging)}
\begin{lstlisting}
    Time slot   0
ld_routine
	Loaded a process at input/proc/p0s, PID: 1 PRIO: 0
Time slot   1
	CPU 1: Dispatched process  1
	Loaded a process at input/proc/p1s, PID: 2 PRIO: 15
Time slot   2
	CPU 0: Dispatched process  2
liballoc:152
--- Page Table Dump ---
print_pgtbl:
 PGD=0x739974001000 P4D=0x739974003000 PUD=0x739974005000 PMD=0x739974007000 PT=0x739974009000
0000000000000000: 0000000000000001
Time slot   3
	Loaded a process at input/proc/p1s, PID: 3 PRIO: 0
liballoc:152
--- Page Table Dump ---
print_pgtbl:
 PGD=0x739974001000 P4D=0x739974003000 PUD=0x739974005000 PMD=0x739974007000 PT=0x739974009000
0000000000000000: 0000000000000001
Time slot   4
libfree:168
--- Page Table Dump ---
print_pgtbl:
 PGD=0x739974001000 P4D=0x739974003000 PUD=0x739974005000 PMD=0x739974007000 PT=0x739974009000
0000000000000000: 0000000000000001
Time slot   5
liballoc:152
--- Page Table Dump ---
print_pgtbl:
 PGD=0x739974001000 P4D=0x739974003000 PUD=0x739974005000 PMD=0x739974007000 PT=0x739974009000
0000000000000000: 0000000000000001
Time slot   6
	Loaded a process at input/proc/p0s, PID: 4 PRIO: 0
libwrite:343
--- Page Table Dump ---
print_pgtbl:
 PGD=0x739974001000 P4D=0x739974003000 PUD=0x739974005000 PMD=0x739974007000 PT=0x739974009000
0000000000000000: 0000000000000001
Time slot   7
	CPU 1: Put process  1 to run queue
	CPU 1: Dispatched process  3
Time slot   8
	CPU 0: Put process  2 to run queue
	CPU 0: Dispatched process  4
Time slot   9
liballoc:152
--- Page Table Dump ---
print_pgtbl:
 PGD=0x73996c001000 P4D=0x73996c003000 PUD=0x73996c005000 PMD=0x73996c007000 PT=0x73996c009000
0000000000000000: 0000000000001001
Time slot  10
liballoc:152
--- Page Table Dump ---
print_pgtbl:
 PGD=0x73996c001000 P4D=0x73996c003000 PUD=0x73996c005000 PMD=0x73996c007000 PT=0x73996c009000
0000000000000000: 0000000000001001
Time slot  11
libfree:168
--- Page Table Dump ---
print_pgtbl:
 PGD=0x73996c001000 P4D=0x73996c003000 PUD=0x73996c005000 PMD=0x73996c007000 PT=0x73996c009000
0000000000000000: 0000000000001001
Time slot  12
liballoc:152
--- Page Table Dump ---
print_pgtbl:
 PGD=0x73996c001000 P4D=0x73996c003000 PUD=0x73996c005000 PMD=0x73996c007000 PT=0x73996c009000
0000000000000000: 0000000000001001
Time slot  13
libwrite:343
--- Page Table Dump ---
print_pgtbl:
 PGD=0x73996c001000 P4D=0x73996c003000 PUD=0x73996c005000 PMD=0x73996c007000 PT=0x73996c009000
0000000000000000: 0000000000001001
	CPU 1: Put process  3 to run queue
	CPU 1: Dispatched process  1
libread:304
Time slot  14
	CPU 0: Put process  4 to run queue
	CPU 0: Dispatched process  3
Time slot  15
Time slot  16
Time slot  17
Time slot  18
	CPU 0: Processed  3 has finished
	CPU 0: Dispatched process  4
libread:304
Time slot  19
	CPU 1: Put process  1 to run queue
	CPU 1: Dispatched process  1
libfree:168
--- Page Table Dump ---
print_pgtbl:
 PGD=0x739974001000 P4D=0x739974003000 PUD=0x739974005000 PMD=0x739974007000 PT=0x739974009000
0000000000000000: 0000000000000001
Time slot  20
Time slot  21
	CPU 1: Processed  1 has finished
	CPU 1: Dispatched process  2
Time slot  22
Time slot  23
Time slot  24
	CPU 0: Put process  4 to run queue
	CPU 0: Dispatched process  4
libfree:168
--- Page Table Dump ---
print_pgtbl:
 PGD=0x73996c001000 P4D=0x73996c003000 PUD=0x73996c005000 PMD=0x73996c007000 PT=0x73996c009000
0000000000000000: 0000000000001001
Time slot  25
	CPU 1: Processed  2 has finished
	CPU 1 stopped
Time slot  26
	CPU 0: Processed  4 has finished
	CPU 0 stopped

\end{lstlisting}
%%%%%%%%%%%%%
\subsubsection{Explain the status of the memory allocation in heap and data segments}

\begin{itemize}
    \item \textbf{Heap Segment:} It is the memory area for initialization (Dynamic Allocation) while the program is running. It has no fixed size and can be expanded.
    
    \item \textbf{Data Segment:} In this simplified simulation model, the "Data Segment" refers to the actual byte values written into the allocated Heap memory frames. While the Heap defines the "container" (virtual addresses and size), the Data represents the "content" stored in the corresponding Physical Frames (RAM).
\end{itemize}

\subsubsection{Chronological Analysis of Memory Allocation}

The following analysis tracks the state of memory allocation across specific time slots where memory intervention occurs. Two distinct processes are identified by their Page Global Directory (PGD) addresses:
\begin{itemize}
    \item \textbf{Process A (p0s):} PGD Address \texttt{0x739974001000}
    \item \textbf{Process B (p1s):} PGD Address \texttt{0x73996c001000}
\end{itemize}
\textbf{Start Explain the status of the memory allocation in heap and data segments.}
\begin{itemize}
    \item\textbf{Time Slot 2: Initial Allocation (Process A)}
    \begin{itemize}
        \item \textbf{Event:} \texttt{liballoc:152} triggered by Process A.
        \item \textbf{Action:} The process requests memory allocation. The OS expands the Heap and creates a new virtual memory region starting at Virtual Address 0.
        \item \textbf{Page Table Dump:} \texttt{00...00: 00...01}
        \item \textbf{Analysis:} The Page Table Entry (PTE) value \texttt{0x1} indicates:
        \begin{itemize}
            \item \textbf{Present Bit (Bit 0):} 1 (Page is in RAM).
            \item \textbf{Frame Page Number (FPN):} 0 (Derived from the upper bits).
        \end{itemize}
    \end{itemize}
        Process A is assigned \textbf{Physical Frame 0}.
    \item \textbf{Time Slot 4: Logical Deallocation (Process A)}
    \begin{itemize}
        \item \textbf{Event:} \texttt{libfree:168}.
        \item \textbf{Action:} Process A calls \texttt{free}.
        \item \textbf{Heap Status:} The memory region is logically marked as "free" and added to the \texttt{vm\_freerg\_list} for future reuse.
    \end{itemize}
    
    \item\textbf{Time Slot 5: Re-allocation (Process A)}
    \begin{itemize}
        \item \textbf{Event:} \texttt{liballoc:152}.
        \item \textbf{Action:} Process A requests allocation again.
        \item \textbf{Heap Status:} The OS checks the \texttt{vm\_freerg\_list}, finds the region freed in Time Slot 4, and reuses it.
        \item \textbf{Mapping:} The mapping remains \texttt{00...01} (Physical Frame 0).
    \end{itemize}
    
    \item \textbf{Time Slot 6: Data Write (Process A)}
    \begin{itemize}
        \item \textbf{Event:} \texttt{libwrite:343}.
        \item \textbf{Action:} Data is written to the allocated address.
        \item \textbf{Data Status:} The actual value is stored in \textbf{Physical Frame 0}.
    \end{itemize}
    
    \item\textbf{Time Slot 9: Allocation for New Process (Process B)}
    \begin{itemize}
        \item \textbf{Event:} \texttt{liballoc:152}.
        \item \textbf{Context Switch:} The PGD changes to \texttt{0x73996c001000}, indicating a context switch to Process B.
        \item \textbf{Page Table Dump:} \texttt{00...00: 00...1001}
        \item \textbf{Analysis:} The PTE value \texttt{0x1001} indicates:
        \begin{itemize}
            \item \textbf{Present Bit:} 1.
            \item \textbf{FPN:} 1 (The value \texttt{0x1000} at bit offset 12 corresponds to index 1).
        \end{itemize}
        Since Frame 0 is occupied by Process A, Process B is assigned the next available resource, \textbf{Physical Frame 1}.
    \end{itemize}
    
    \item \textbf{Time Slot 11 \& 13: Free and Write (Process B)}
    \begin{itemize}
        \item \textbf{Time Slot 11 (Free):} Process B frees its memory region. The region enters Process B's private free list.
        \item \textbf{Time Slot 13 (Write):} Process B writes data. This data is stored in \textbf{Physical Frame 1}.
    \end{itemize}


\item \textbf{Time Slot 19 \& 24: Final Deallocation}
Both processes perform their final \texttt{libfree} operations (Slot 19 for Process A, Slot 24 for Process B), releasing their respective logical memory regions.
\end{itemize}


\subsubsection{Summary of Result Flow}

The table below summarizes the correlation between Time Slots, Processes, and Physical Memory status.

\begin{longtable}{@{}cllll@{}}
\toprule
\textbf{Slot} & \textbf{PGD (Process)} & \textbf{Event} & \textbf{Physical Frame} & \textbf{Logical Heap Status} \\ \midrule
\endhead
2 & ...7400 (Proc A) & Alloc & Frame 0 & New Region created \\
4 & ...7400 (Proc A) & Free & Frame 0 & Region moved to Free List \\
5 & ...7400 (Proc A) & Alloc & Frame 0 & Region reused from Free List \\
6 & ...7400 (Proc A) & Write & Frame 0 & Data written to Frame 0 \\
9 & ...6c00 (Proc B) & Alloc & \textbf{Frame 1} & New Region created \\
11 & ...6c00 (Proc B) & Free & Frame 1 & Region moved to Free List \\
13 & ...6c00 (Proc B) & Write & Frame 1 & Data written to Frame 1 \\ \bottomrule
\caption{Memory Allocation Flow Summary}
\label{tab:mem-flow}
\end{longtable}
%%%%%%%%%%%%%


\pagebreak

	\section{Answer question}
	\subsection{Question 1}
	\textbf{Question:} What is the mechanism to pass a complex argument to a system call using the limited registers?

\vspace{0.5cm}

    \noindent\textbf{Answer:}
    
	The mechanism used to pass a complex argument to a system call is Indirection via pointers. Since CPU registers are very limited in number and size, they cannot hold large or complex data directly. Instead, the user-space program places the data in its own memory and passes only the memory address (pointer) of that data in a register. When the system call is invoked, the kernel receive the pointer from the register. But this method have a problem that the passed address maybe wrong or bad address (pointing to kernel memory or unmapped memory) leading to the kernel could crash. To handle this problem, the kernel check if the address provided is actually within the User space memory area. Then the kernel copy the data from User space into Kernel Space buffer, the Kernel perform on its own safe copy of the data.
	
	\subsection{Question 2}
	\textbf{Question:} Which mechanisms does the operating system use to manage system calls that become unresponsive?

\vspace{0.5cm}

    \noindent\textbf{Answer:}
    
	Operating systems use a multi-layered approach to manage system calls that become unresponsive. The mechanisms used to manage this fall into three categories:
	\begin{itemize}
		\item Interruption Mechanisms: when the system call blocks, the operating system provides ways for the user or the application to forcefully break. In Linux most of blocking system calls place the process in a task interruptible state, when the process receives a signal like Ctrl + C, the kernel prematurely wakes the process.
		\item Timeout/Watchdogs: The kernel monitors itself to ensure that a single stuck system call does not freeze the entire computer. Many blocking system calls accept a timeout parameter. The kernel sets an internal timer; if the condition is not satisfied before the timer expires, the call returns with an error. In addition, the operating system runs a background kernel thread that scans all process in every constant time (120 seconds in Linux), if it find a process stuck in Uninterruptible Sleep (D state) for more than this duration, it logs a "Hung Task" error to the kernel log with a stack trace.
		\item Asynchronous Avoidance: Modern operating systems encourage developers to avoid "blocking" system calls entirely to prevent unresponsiveness. Applications can set file descriptors to non-blocking mode (O\_NONBLOCK). Its mean that if a system call would freeze, it returns immediately with an error instead of waiting. In addition, there are some API (Application Programming Interface) like io\_uring, epoll,.. in Linux that allow applications to submit system calls to a queue and retrieve results later, ensuring the main application thread never actually block.
	\end{itemize}
    
	\subsection{Question 3}
    \textbf{Question:} Evaluate the comparative advantages of the scheduling algorithm implemented in this assignment in relation to other scheduling algorithms you have learned. What is the complexity of this algorithm?

\vspace{0.5cm}

\noindent\textbf{Answer:}

The scheduling algorithm implemented in this assignment is a \textbf{Non-Feedback Multilevel Queue (MLQ)} augmented with a \textbf{Slot-based Round Robin} mechanism for inter-queue dispatching. The design employs a static array of priority queues, where each priority level is assigned a specific execution quota (slot). Based on the theoretical framework provided in \textit{Operating System Concepts (10th Edition)} and the specific logic in \texttt{sched.c}, we present a detailed comparative evaluation and complexity analysis below.

% --- SECTION 1: COMPARATIVE ADVANTAGES ---
\begin{enumerate}[label=\textbf{\arabic*.}]
    \item \textbf{Comparative Evaluation against Standard Algorithms}
    
    \begin{enumerate}[label=\textbf{\alph*.}]
% --- 1. FCFS ---
    \item \textbf{Comparison with First-Come, First-Served (FCFS)}
    
    \begin{itemize}
        \item \textbf{Theoretical Background:} 
        FCFS is the most basic non-preemptive scheduling algorithm. The CPU is allocated to the process at the head of the ready queue. The critical drawback identified in operating system theory is the \textit{"Convoy Effect"}. This phenomenon occurs when a CPU-intensive process occupies the processor for an extended period, causing all subsequent I/O-bound processes to wait in the ready queue. This leads to extremely low I/O device utilization and a high Average Waiting Time.
        
        \item \textbf{Assignment Implementation Analysis:}
        The implemented algorithm in the assignment is inherently \textbf{Preemptive}.
        \begin{itemize}
            \item \textit{Mechanism:} The simulation framework in \texttt{os.c} enforces a \texttt{time\_slot}. Even if a process has not completed its CPU burst, it is forced to yield the CPU and is placed back into the \texttt{mlq\_ready\_queue} via \texttt{put\_mlq\_proc()}.
            \item \textit{Code Evidence:} In \texttt{sched.c}, the scheduler manages \texttt{slot[i]}. When a process consumes its time slice, the scheduler is invoked again to pick a new candidate.
        \end{itemize}
        
        \item \textbf{Comparative Advantage:}
        Our Slot-based MLQ algorithm completely eliminates the Convoy Effect. By forcing time-sharing (Round Robin within queues), short processes and I/O-bound processes are guaranteed frequent CPU access. This significantly improves the system's \textbf{Responsiveness}, making it suitable for interactive environments, whereas FCFS is strictly limited to Batch Systems.
    \end{itemize}

   % --- 2. SJF ---
    \item \textbf{Comparison with Shortest-Job-First (SJF)}
    
    \begin{itemize}
        \item \textbf{Theoretical Background (Optimality vs. Prediction):} 
        SJF (or strictly, \textit{Shortest-Next-CPU-Burst}) is theoretically proven to yield the minimum average waiting time. However, it suffers from a fundamental implementation flaw: the OS cannot know the length of the next CPU burst ($\tau_{n+1}$) in advance. 
        
        To implement SJF, the system relies on \textbf{Exponential Averaging} to predict the future based on past behavior using the formula:
        \begin{equation}
            \tau_{n+1} = \alpha t_n + (1 - \alpha)\tau_n
        \end{equation}
        Where $t_n$ is the most recent burst, $\tau_n$ is the past prediction, and $\alpha$ ($0 \le \alpha \le 1$) is the weighting factor.

        \item \textbf{Mechanism and Properties:}
        Functionally, this mechanism acts as a \textbf{Low-Pass Filter}, smoothing out short-term fluctuations (noise) to reveal the underlying trend of the process.
        \begin{itemize}
            \item \textit{Exponential Decay:} Expanding the formula ($\tau_{n+1} = \alpha t_n + (1-\alpha)\alpha t_{n-1} + \dots$) reveals that the weight of past bursts decays exponentially. This ensures recent history dominates the prediction while distant history fades away.
            \item \textit{Weighting Control:} The parameter $\alpha$ controls stability. Typically $\alpha = 0.5$ is used to balance recent and past history.
        \end{itemize}

        \item \textbf{Scenario Analysis (When is it Good vs. Bad?):}
        \begin{itemize}
            \item \textbf{Effective Scenario (Stationary Behavior):} The algorithm excels when processes exhibit \textbf{Locality of Reference} (e.g., a process staying in a consistent CPU-bound phase). The formula quickly converges to the true average, and the smoothing effect prevents the scheduler from overreacting to minor random variations in burst length.
            
            \item \textbf{Degraded Scenario (Instability and Seasonality):}
            \begin{itemize}
                \item \textit{Prediction Lag:} If a process behavior shifts suddenly (e.g., I/O-bound $\to$ CPU-bound), the prediction lags behind due to the inertia of history, leading to suboptimal scheduling during the transition.
                \item \textit{Seasonality Failure:} Simple exponential smoothing fails to capture cyclic patterns (e.g., 3 short bursts followed by 1 long burst). The algorithm will constantly "chase" the data—predicting high after a peak and low after a valley—always being one step behind the true cycle.
                \item \textit{Outlier Sensitivity:} If $\alpha \approx 1$, the system lacks stability. A single random long burst (outlier) causes the prediction to spike, unfairly penalizing the process in the next cycle.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Assignment Implementation Analysis (Static Determinism):}
        Our implemented algorithm prioritizes \textbf{Determinism} and \textbf{Feasibility} over theoretical optimality.
        \begin{itemize}
            \item \textit{Mechanism:} Calculating exponential averages requires floating-point arithmetic or complex fixed-point math inside the kernel, which incurs per-process overhead. Instead of guessing $\tau_{n+1}$, our system relies on the explicit \texttt{proc->prio} value defined in the PCB.
            \item \textit{Code Evidence:} The selection logic in \texttt{sched.c} (\texttt{get\_mlq\_proc}) utilizes simple integer comparisons and the \texttt{slot} mechanism. This avoids the runtime cost of prediction and ensures predictable behavior ($O(1)$ decision complexity).
        \end{itemize}
        
        \item \textbf{Comparative Evaluation:} 
        Unlike SJF, which can degrade if $\alpha$ is poorly tuned or if processes exhibit erratic/seasonal burst patterns, our Slot-based MLQ guarantees that high-priority processes always run first. Simultaneously, the slot mechanism prevents the starvation that strict priority scheduling usually suffers from, providing a balanced trade-off between responsiveness and fairness without the computational cost of prediction.
    \end{itemize}
    % --- 3. Priority Scheduling (Crucial Point) ---
    \item \textbf{Comparison with Pure Priority Scheduling}
    
    \begin{itemize}
        \item \textbf{Theoretical Background:} 
        In standard Priority Scheduling, the CPU is always allocated to the highest priority process. The fatal flaw of this design is \textit{"Indefinite Blocking"} or \textit{"Starvation"}. If a steady stream of high-priority processes arrives, low-priority processes may never execute. The standard solution is "Aging" (gradually increasing the priority of waiting processes), which adds complexity to the system.
        
        \item \textbf{Assignment Implementation Analysis (The Slot Mechanism):}
        This is the most innovative aspect of the assignment's design. It solves starvation \textbf{without} changing process priorities (Aging).
        \begin{itemize}
            \item \textit{Code Logic:}
            \begin{lstlisting}[language=C, basicstyle=\ttfamily\scriptsize]
// sched.c : get_mlq_proc()
if (slot[i] > 0) {
    proc = dequeue(...);
    slot[i]--; // Consume budget
    break;     // Dispatch immediate
} else {
    slot[i] = MAX_PRIO - i; // Refill budget
    continue; // SKIP this queue, force check lower priority
}
            \end{lstlisting}
            \item \textit{Operational Theory:} The system assigns a "CPU Budget" to each priority level based on the formula $Budget = MAX\_PRIO - Prio$. 
            \item \textit{Example:} Priority 0 gets 140 slots. Priority 1 gets 139 slots. Even if Priority 0 has infinite tasks, after 140 dispatches, the \texttt{slot[0]} becomes 0. The scheduler is mathematically forced to skip Priority 0 and serve Priority 1.
        \end{itemize}
        
        \item \textbf{Comparative Advantage:}
        The implementation achieves \textbf{Starvation Freedom} through a "Share-based" approach. It guarantees that every priority level, no matter how low, eventually receives a turn when higher levels exhaust their quotas. This is a robust and fairness-oriented improvement over standard Priority Scheduling.
    \end{itemize}

    % --- 4. Standard MLQ ---
    \item \textbf{Comparison with Standard Multilevel Queue (Partitioned)}
    
    \begin{itemize}
        \item \textbf{Theoretical Background:} 
        Standard MLQ partitions the ready queue into distinct groups, typically "Foreground" (Interactive) and "Background" (Batch). The scheduling between queues is often Fixed-Priority (leading to starvation) or Time-Slicing with coarse granularity (e.g., 80\% CPU for Foreground, 20\% for Background).
        
        \item \textbf{Assignment Implementation Analysis:}
        Our implementation offers a \textbf{Fine-Grained Proportional Share}.
        \begin{itemize}
            \item \textit{Granularity:} Instead of just 2 or 3 partitions, the system supports \texttt{MAX\_PRIO} (140) distinct levels.
            \item \textit{Distribution:} The slot allocation decreases linearly ($140, 139, \dots, 1$).
        \end{itemize}
        
        \item \textbf{Comparative Advantage:}
        This linear distribution creates a smoother degradation of service. It allows the OS to differentiate processes with high precision (e.g., a process with priority 10 is slightly favored over 11), providing more flexibility than the rigid Foreground/Background binary of standard MLQ.
    \end{itemize}

    % --- 5. MLFQ ---
    \item \textbf{Comparison with Multilevel Feedback Queue (MLFQ)}
    
    \begin{itemize}
        \item \textbf{Theoretical Background:} 
        MLFQ is considered the most general and powerful scheduling algorithm. Its key feature is \textit{Feedback}: processes move between queues. A process using too much CPU moves down; a process waiting too long moves up (Aging). This allows the OS to learn the nature of the process (I/O-bound vs CPU-bound) dynamically.
        
        \item \textbf{Assignment Implementation Limitation:}
        The assignment specification explicitly states a non-feedback design.
        \begin{itemize}
            \item \textit{Code Evidence:} In \texttt{put\_mlq\_proc()}, a process is always enqueued back to \\
            \texttt{mlq\_ready\_queue[proc->prio]}. The \texttt{prio} field is never modified by the scheduler.
        \end{itemize}
        
        \item \textbf{Disadvantage/Trade-off:}
        The lack of feedback makes the system less adaptive. If a high-priority process becomes CPU-intensive, it continues to enjoy a large slot quota (140 slots), potentially degrading overall system throughput compared to an MLFQ which would demote such a process. However, this simplifies the implementation significantly and reduces the overhead of priority recalculation.
    \end{itemize}
    \end{enumerate} 

    % --- SECTION 2: COMPLEXITY ANALYSIS ---
    \item \textbf{Algorithmic Complexity Analysis}
    
    The performance of the scheduler is determined by the operations on the \texttt{queue\_t} structure (array-based) and the search logic in \texttt{sched.c}. Let $N$ be the number of processes in a specific queue, and $K$ be the number of priority levels ($K = \texttt{MAX\_PRIO} = 140$).

    \begin{itemize}
        \item \textbf{Enqueue Operation ($O(1)$):}
        \begin{itemize}
            \item \textit{Code Reference:} \texttt{queue.c: enqueue()}.
            \item \textit{Analysis:} The function inserts a process at the tail of the array: \texttt{q->proc[q->size] = proc}. This requires direct index access and incrementing the size counter.
            \item \textit{Complexity:} Constant time, $O(1)$.
        \end{itemize}

        \item \textbf{Dequeue Operation ($O(N)$):}
        \begin{itemize}
            \item \textit{Code Reference:} \texttt{queue.c: dequeue()}.
            \item \textit{Analysis:} The scheduler retrieves the process at the head (index 0). Because the queue is implemented as a contiguous memory block (array), removing the first element creates a "gap". The loop \texttt{for (i = 0; i < q->size - 1; i++)} is required to shift all remaining $N-1$ elements to the left to preserve data contiguity.
            \item \textit{Complexity:} Linear time relative to queue size, $O(N)$.
        \end{itemize}

        \item \textbf{Scheduler Selection Logic ($O(K + N)$):}
        \begin{itemize}
            \item \textit{Code Reference:} \texttt{sched.c: get\_mlq\_proc()}.
            \item \textit{Analysis:} The routine employs a linear search across priority levels: \texttt{for (int i = 0; i < MAX\_PRIO; i++)}. In the worst-case scenario (e.g., when the system is idle or all high-priority slots are exhausted), the loop iterates $K$ times. Upon finding a valid queue, it calls \texttt{dequeue()}.
            \item \textit{Calculation:} $Cost = Search\_Cost + Retrieval\_Cost = O(K) + O(N)$.
            \item \textbf{Total Complexity:} $O(K + N)$.
         \end{itemize}
    \end{itemize}

\end{enumerate}

	\subsection{Question 4}
    \textbf{Question:} Discuss the architectural and operational implications of implementing multiple memory segments in this simple OS.

    \vspace{0.5cm}

    \noindent\textbf{Answer:}
    
Implementing multiple memory segments fundamentally structures how the OS manages the virtual address space on top of the physical paging system. The implications can be divided into architectural changes and operational behaviors.

    % Bắt đầu danh sách cấp 1 (1., 2.)
    \begin{enumerate}[label=\textbf{\arabic*.}]

        % --- 1. Architectural Implications ---
        \item \textbf{Architectural Implications}
        
        From an architectural perspective, this adds a logical organization layer above the raw paging mechanism:
        
        \begin{enumerate}[label=\textbf{\alph*.}]
            \item \textbf{Data Structure Transformation:}
            In the design, a segment is represented by a \texttt{vm\_area\_struct} inside the per-process \texttt{mm\_struct}. Implementing multiple segments means that instead of a single VMA (current \texttt{vm\_id = 0} for "data/heap"), \texttt{mm->mmap} becomes a linked list of VMAs. For example:
            \begin{itemize}
                \item \texttt{vm\_id = 0}: Code / Read-only data
                \item \texttt{vm\_id = 1}: Heap
                \item \texttt{vm\_id = 2}: Stack
                \item \texttt{vm\_id = 3}: Shared regions, etc.
            \end{itemize}
            
            \item \textbf{Logical Abstraction Layer:}
            Architecturally, this creates a logical layer on top of the 5-level page tables:
            \begin{itemize}
                \item The \textbf{5-level paging} (PGD--P4D--PUD--PMD--PT) remains unchanged and is solely responsible for translating virtual page numbers to physical frames.
                \item The \textbf{Segmentation layer} decides \textit{which} virtual ranges exist and \textit{what} they are used for. Each VMA tracks its virtual interval \texttt{[vm\_start, vm\_end)}, current growth point \texttt{sbrk}, and a free-region list for local holes.
            \end{itemize}

            \item \textbf{Structured Extensibility:}
            This makes the address space structured and extensible. Different regions can later be assigned different protection policies (e.g., code as read-only, stack as non-executable, or shared segments mapped into multiple processes) while still sharing the same page table root (\texttt{mm->pgd}).
        \end{enumerate}

        % --- 2. Operational Implications ---
        \item \textbf{Operational Implications}
        
        On the operational side, supporting multiple segments changes the workflow of memory operations:

        \begin{enumerate}[label=\textbf{\alph*.}]
            \item \textbf{Segment-Aware Operations:}
            All core memory functions (\texttt{\_\_alloc}, \texttt{\_\_free}, \texttt{\_\_read}, \texttt{\_\_write}) become segment-aware. They take a \texttt{vmaid} parameter to select the correct VMA via \texttt{get\_vma\_by\_num(mm, vmaid)} before performing any work. The allocator only searches and updates the free-region list of that specific segment, and \texttt{inc\_vma\_limit} grows only that targeted VMA when more virtual space is needed.

            \item \textbf{Bookkeeping vs. Isolation:}
            There is a trade-off involving increased bookkeeping for better isolation. Every time the kernel extends a segment, it must check for overlaps with other VMAs (\texttt{validate\_overlap\_vm\_area}) before calling \texttt{vm\_map\_ram} to map new pages. While this adds overhead (walking the VMA list, extra validation), it guarantees that distinct segments (e.g., Heap vs. Stack) do not collide and grow independently.

            \item \textbf{Contextual Paging and Swapping:}
            While low-level page-table operations in \texttt{mm64.c} (allocating frames, filling PTEs, swapping victim pages) remain structurally the same, the context of each page becomes clearer. The OS knows whether a page belongs to code, heap, or stack based on the VMA it falls into. This facilitates segment-specific policies (e.g., preventing code pages from being swapped out or prioritizing stack pages) without altering the core paging algorithms.
        \end{enumerate}

    \end{enumerate}

    \vspace{0.5cm}
    In summary, while multiple segments slightly increase complexity in the memory manager, the OS gains a much more organized virtual address space and a natural architectural hook to implement protection and per-segment policies on top of the underlying 5-level paging mechanism.    
    
	\subsection{Question 5}
\textbf{Question:} What will happen if we divide the address to more than 2 levels in the paging memory management system? 

\vspace{0.5cm}

\noindent\textbf{Answer:}

Based on the Simple OS implementation, dividing the address into more than 2 levels fundamentally changes the paging structure and introduces several important consequences:

% Bắt đầu danh sách cấp 1 (1., 2., 3.)
\begin{enumerate}[label=\textbf{\arabic*.}]
	
	% --- 1. Current 2-Level Implementation ---
	\item \textbf{Current 2-Level Implementation}
	
	In the Simple OS's 2-level paging system (defined in \texttt{common.h}), the 20-bit address space is divided as follows:
	\begin{itemize}
		\item \textbf{First level (Segment):} 5 bits (\texttt{FIRST\_LV\_LEN}) = 32 entries
		\item \textbf{Second level (Page):} 5 bits (\texttt{SECOND\_LV\_LEN}) = 32 entries
		\item \textbf{Offset:} 10 bits (\texttt{OFFSET\_LEN}) = 1KB pages
	\end{itemize}
	This creates a hierarchical structure with \texttt{page\_table\_t} containing pointers to \texttt{trans\_table\_t} entries, requiring 2 memory accesses for address translation. The total virtual address space is limited to 1 MB (2$^{20}$ bytes).
	
	% --- 2. 5-Level Paging (64-bit Implementation) ---
	\item \textbf{5-Level Paging (64-bit Implementation)}
	
	When moving to 5-level paging in the 64-bit mode (\texttt{mm64.h} and \texttt{mm64.c}), the address is divided into 5 distinct levels:
	\begin{itemize}
		\item \textbf{Level 5 - PGD (Page Global Directory):} bits 56-48 (9 bits = 512 entries)
		\item \textbf{Level 4 - P4D (Page Level 4 Directory):} bits 47-39 (9 bits = 512 entries)
		\item \textbf{Level 3 - PUD (Page Upper Directory):} bits 38-30 (9 bits = 512 entries)
		\item \textbf{Level 2 - PMD (Page Middle Directory):} bits 29-21 (9 bits = 512 entries)
		\item \textbf{Level 1 - PT (Page Table):} bits 20-12 (9 bits = 512 entries)
		\item \textbf{Offset:} bits 11-0 (12 bits = 4KB pages)
	\end{itemize}
	This expands the virtual address space from 1 MB (20-bit) to 128 PiB (57-bit canonical address space), as implemented in \texttt{mm64.c}.
	
	% --- 3. Key Consequences of Multi-Level Paging ---
	\item \textbf{Key Consequences of Multi-Level Paging}
	
	% Bắt đầu danh sách cấp 2 (a., b., c.)
	\begin{enumerate}[label=\textbf{\alph*.}]
		% a. Address Space Expansion
		\item \textbf{Address Space Expansion:} 
		The 5-level scheme supports up to 128 petabytes of virtual memory, compared to the 2-level system's 1 MB limit. The function \texttt{get\_pd\_from\_address()} in \texttt{mm64.c} (lines 149-157) extracts all 5 directory indices from a single address using bit masks and shifts, enabling this massive expansion. Each level uses 9 bits, allowing 512 entries per directory table.
		
		\item \textbf{Increased Translation Overhead:}
		Each address translation requires traversing all 5 levels (PGD $\rightarrow$ P4D $\rightarrow$ PUD $\rightarrow$ PMD $\rightarrow$ PT), requiring 5 memory accesses instead of 2. The implementation in \texttt{get\_page\_table\_entry()} demonstrates this hierarchical traversal. Modern CPUs use Translation Lookaside Buffers (TLB) to cache recent translations, mitigating this overhead in practice.
		
		\item \textbf{Page Table Entry Operations Complexity:}
		All PTE operations must traverse the full 5-level hierarchy:
		\begin{itemize}
			\item \texttt{pte\_set\_fpn()} (lines 192-208): Walks 5 levels to set frame number
			\item \texttt{pte\_set\_swap()} (lines 171-187): Walks 5 levels to set swap information
			\item \texttt{pte\_get\_entry()} (lines 211-218): Walks 5 levels to read PTE
			\item \texttt{vmap\_pgd\_memset()} (lines 234-246): Must walk 5 levels for each page mapped
		\end{itemize}
		Each operation calls \texttt{get\_page\_table\_entry()} which performs the full traversal, allocating intermediate directories as needed.
		
		\item \textbf{Implementation Complexity:}
		The 5-level implementation is significantly more complex than the 2-level system:
		\begin{itemize}
			\item \texttt{init\_mm()} (lines 358-386 in mm64.c): Initializes the root PGD pointer as NULL, with VM area structures
			\item \texttt{get\_page\_table\_entry()}: Implements recursive tree traversal with dynamic allocation
			\item \texttt{print\_pgtbl\_recursive()} (lines 546-595 in mm64.c): Recursive function to traverse and print all 5 levels
			\item Address extraction in \texttt{get\_pd\_from\_address()}: Complex bit manipulation across 5 levels
		\end{itemize}
		The code structure shows separate directory types (\texttt{pgd\_t}, \texttt{p4d\_t}, \texttt{pud\_t}, \texttt{pmd\_t}, \texttt{pt\_t}) defined in \texttt{os-mm.h}, each containing 512 entries.
	\end{enumerate}
\end{enumerate}
In summary, moving from 2-level to 5-level paging in this Simple OS enables massive address space expansion (from 1 MB to 128 PiB) but requires more complex address parsing, slower translation (5 memory accesses vs. 2), and sophisticated memory management strategies.

\subsection{Question 6}

\textbf{Question:}What are the advantages and disadvantages of segmentation with paging?

\vspace{0.5cm}

\noindent\textbf{Answer:}

In the Simple OS implementation, segmentation with paging is realized through the \texttt{vm\_area\_struct} (VM areas) acting as logical segments, which are then divided into fixed-size pages managed by the page table hierarchy. This hybrid approach combines the benefits of both memory management techniques.

% Bắt đầu danh sách cấp 1 (1., 2., 3., 4.)
\begin{enumerate}[label=\textbf{\arabic*.}]
	
	% --- 1. Implementation in Simple OS ---
	\item \textbf{Implementation in Simple OS}
	
	The OS uses \texttt{vm\_area\_struct} (defined in \texttt{os-mm.h}) as segment-like structures, each containing:
	\begin{itemize}
		\item \texttt{vm\_id}: Identifier for the VM area (logical segment identifier)
		\item \texttt{vm\_start} and \texttt{vm\_end}: Boundaries defining the logical segment's virtual address range
		\item \texttt{sbrk}: Break pointer for dynamic memory allocation within the segment (heap growth)
		\item \texttt{vm\_freerg\_list}: Linked list of free regions within the segment for efficient reuse
		\item \texttt{vm\_mm}: Back-pointer to the \texttt{mm\_struct} for accessing page tables
		
	\end{itemize}
	Each VM area is then paged using the hierarchical page table structure (\texttt{mm\_struct->pgd} in 64-bit mode, as defined in \texttt{os-mm.h}), where pages are mapped to non-contiguous physical frames. The symbol table (\texttt{symrgtbl}) tracks memory regions (variables) within VM areas, similar to how segments track logical units. The \texttt{mm\_struct->mmap} field maintains a linked list of all VM areas for a process.
	
	% --- 2. Disadvantages ---
	\item \textbf{Disadvantages}
	
	\begin{enumerate}[label=\textbf{\alph*.}]
		\item \textbf{Increased Translation Complexity}
		
		Segmentation with paging requires two conceptual steps for address translation: first identifying which segment (VM area) contains the address, then translating the address through the page table hierarchy. In systems with multi-level page tables (such as 64-bit architectures), address translation involves traversing multiple levels of page directories (PGD, P4D, PUD, PMD, PT), potentially requiring multiple memory accesses for each translation. This increases the complexity and latency of address translation compared to simpler memory management schemes.
		
		In the Simple OS implementation, this complexity manifests in the \texttt{get\_pd\_from\_address()} function (lines 149-157 of \texttt{mm64.c}), which extracts all 5 page directory indices from a virtual address:
		\begin{lstlisting}
int get_pd_from_address(addr_t addr, addr_t *pgd, addr_t *p4d, 
						addr_t *pud, addr_t *pmd, addr_t *pt)
{
	*pgd = (addr & PAGING64_ADDR_PGD_MASK) >> PAGING64_ADDR_PGD_LOBIT;
	*p4d = (addr & PAGING64_ADDR_P4D_MASK) >> PAGING64_ADDR_P4D_LOBIT;
	*pud = (addr & PAGING64_ADDR_PUD_MASK) >> PAGING64_ADDR_PUD_LOBIT;
	*pmd = (addr & PAGING64_ADDR_PMD_MASK) >> PAGING64_ADDR_PMD_LOBIT;
	*pt = (addr & PAGING64_ADDR_PT_MASK) >> PAGING64_ADDR_PT_LOBIT;
	return 0;
}
		\end{lstlisting}
		Additionally, the \texttt{get\_page\_table\_entry()} function (lines 100-144 of \texttt{mm64.c}) must walk through all 5 levels of the page table hierarchy, potentially requiring 5 memory accesses to reach the final page table entry.
		
		\item \textbf{Memory Overhead for Management Structures}
		
		Segmentation with paging requires maintaining both segment management structures (VM areas) and page table structures simultaneously. Each process must maintain metadata for segments (segment descriptors, boundaries, free region lists) as well as hierarchical page tables. Each level of the page table hierarchy consumes memory (typically 4KB per directory level in 64-bit systems). Additionally, segment structures, free region lists, and symbol tables add overhead. This dual-structure approach consumes more memory than a pure paging system, which only needs page tables.
		
		In the Simple OS implementation, this memory overhead is evident in the structure definitions in \texttt{os-mm.h}:
		\begin{lstlisting}
struct mm_struct {
	struct pgd_t *pgd;              // 4KB per directory level
	struct vm_area_struct *mmap;    // Linked list of VM areas
	struct vm_rg_struct symrgtbl[PAGING_MAX_SYMTBL_SZ]; // 30 entries
	struct pgn_t *fifo_pgn;         // Page number list
};

struct vm_area_struct {
	// ... fields ...
	struct vm_rg_struct *vm_freerg_list; // Free region list per VM area
	struct vm_area_struct *vm_next;      // Linked list overhead
};
		\end{lstlisting}
		The initialization in \texttt{init\_mm()} (\texttt{mm64.c}, lines 357-375) shows all these structures being allocated for each process.
		
		\item \textbf{Complex Overlap Validation}
		
		In segmentation with paging, the system must ensure that segments do not overlap in the virtual address space. This requires validating that new segment allocations do not conflict with existing segments. As the number of segments grows, this validation becomes more expensive, requiring O(n) operations where n is the number of segments. This overhead occurs during memory allocation operations, potentially impacting performance.
		
		In the Simple OS implementation, the \texttt{validate\_overlap\_vm\_area()} function in \texttt{mm-vm.c} (lines 76-102) demonstrates this overhead:
		\begin{lstlisting}
int validate_overlap_vm_area(struct pcb_t *caller, int vmaid,
								addr_t vmastart, addr_t vmaend)
{
	struct vm_area_struct *vma = caller->mm->mmap;
	
	// Must traverse entire linked list
	while (vma != NULL) {
		if (vma->vm_id != vmaid) {
			// Check for overlap: O(n) comparison
			if ((vmastart < vma->vm_end) && (vma->vm_start < vmaend)) {
				return -1; // Overlap detected
			}
		}
		vma = vma->vm_next;
	}
	return 0;
}
		\end{lstlisting}
		This function is called in \texttt{inc\_vma\_limit()} (line 138 of \texttt{mm-vm.c}) and during memory allocation operations, adding overhead to every segment expansion.
	\end{enumerate}
	
	% --- 3. Advantages ---
	\item \textbf{Advantages}
	
	\begin{enumerate}[label=\textbf{\alph*.}]
		\item \textbf{Eliminates External Fragmentation}
		
		By combining segmentation with paging, the system eliminates external fragmentation that plagues pure segmentation schemes. Segments are divided into fixed-size pages, which are then mapped to physical frames. Since pages are of fixed size, any available frame can be used, and segments can span multiple non-contiguous physical frames. This eliminates the need for contiguous physical memory allocation, which is a major limitation of pure segmentation.
		
		In the Simple OS implementation, the \texttt{alloc\_pages\_range()} function (lines 283-317 of \texttt{mm64.c}) allocates frames individually and links them via \texttt{framephy\_struct}:
		\begin{lstlisting}
addr_t alloc_pages_range(struct pcb_t *caller, int req_pgnum,
						struct framephy_struct **frm_lst)
{
	// Allocates frames individually and links them
	for (pgit = 0; pgit < req_pgnum; pgit++) {
		MEMPHY_get_freefp(caller->krnl->mram, &fpn);
		// Create framephy_struct node and link to list
	}
}
		\end{lstlisting}
		The \texttt{vm\_map\_ram()} function (lines 322-337 of \texttt{mm64.c}) coordinates frame allocation and page mapping:
		\begin{lstlisting}
addr_t vm_map_ram(struct pcb_t *caller, addr_t astart, addr_t aend,
				addr_t mapstart, int incpgnum, struct vm_rg_struct *ret_rg)
{
	// Step 1: Allocate physical frames (paging concern)
	ret_alloc = alloc_pages_range(caller, incpgnum, &frm_lst);
	
	// Step 2: Map pages to virtual addresses (paging concern)
	vmap_page_range(caller, mapstart, incpgnum, frm_lst, ret_rg);
	
	return 0;
}
		\end{lstlisting}
		The \texttt{framephy\_struct} is defined in \texttt{os-mm.h} (lines 155-162) and manages non-contiguous frames.
		
		\item \textbf{Logical Memory Organization}
		
		Segmentation with paging provides logical organization of memory by maintaining segments that represent different logical units (code, data, heap, stack). This allows the operating system to treat different memory regions as separate entities with distinct properties, while still using paging for efficient physical memory management. This logical organization simplifies memory protection, sharing, and management compared to pure paging systems.
		
		In the Simple OS implementation, functions like \texttt{get\_vma\_by\_num()} (lines 12-31 of \texttt{mm-vm.c}) traverse the VM area list to find segments by ID:
		\begin{lstlisting}
struct vm_area_struct *get_vma_by_num(struct mm_struct *mm, int vmaid)
{
	struct vm_area_struct *pvma = mm->mmap;
	while (pvma != NULL) {
		if (pvma->vm_id == vmaid)
		return pvma;
		pvma = pvma->vm_next;
	}
	return NULL;
}
		\end{lstlisting}
		The \texttt{validate\_overlap\_vm\_area()} function (lines 76-102 of \texttt{mm-vm.c}) ensures distinct logical regions don't overlap.
		
		\item \textbf{Efficient Handling of Large Segments}
		
		Segmentation with paging allows segments to grow dynamically without requiring contiguous physical memory. Since segments are divided into pages, and pages can be allocated from any available frames, large segments can be created and expanded efficiently. This is particularly beneficial for dynamic data structures like heaps that grow over time, as they don't need to be pre-allocated or relocated when they expand.
		
		In the Simple OS implementation, the \texttt{inc\_vma\_limit()} function (lines 110-168 of \texttt{mm-vm.c}) demonstrates how large VM areas can grow dynamically:
		\begin{lstlisting}
int inc_vma_limit(struct pcb_t *caller, int vmaid, addr_t inc_sz)
{
	// Calculate number of pages needed
	int incnumpage = DIV_ROUND_UP(inc_amt, PAGING_PAGESZ);
	
	// Allocate and map new pages
	vm_map_ram(caller, area->rg_start, area->rg_end, old_sbrk,
	incnumpage, newrg);
	
	// Update sbrk pointer
	cur_vma->sbrk += inc_sz;
}
		\end{lstlisting}
		The function calculates page-aligned size, calls \texttt{vm\_map\_ram()} which allocates frames via \texttt{alloc\_pages\_range()} and maps them via \texttt{vmap\_page\_range()}, then updates the \texttt{sbrk} pointer.
		
		\item \textbf{Reduced Internal Fragmentation}
		
		Segmentation with paging can reduce internal fragmentation within segments by maintaining free region lists that track freed memory. When memory is freed within a segment, it can be reused for subsequent allocations within the same segment, rather than always allocating new pages. This reuse of freed memory reduces wasted space within segments, improving memory utilization compared to systems that always allocate new pages.
		
		In the Simple OS implementation, the \texttt{get\_free\_vmrg\_area()} function (lines 384-437 of \texttt{libmem.c}) searches the free list for regions large enough to satisfy allocation requests:
		\begin{lstlisting}
int get_free_vmrg_area(struct pcb_t *caller, int vmaid, int size,
						struct vm_rg_struct *newrg)
{
	struct vm_rg_struct *rgit = cur_vma->vm_freerg_list;
	while (rgit != NULL) {
		if (rgit->rg_start + size <= rgit->rg_end) {
			// Found fit - reuse existing free region
			newrg->rg_start = rgit->rg_start;
			newrg->rg_end = rgit->rg_start + size;
			// Update free node efficiently
		}
		rgit = rgit->rg_next;
	}
}
		\end{lstlisting}
		The \texttt{\_\_alloc()} function (lines 38-100 of \texttt{libmem.c}) first attempts to reuse free regions before expanding the heap:
		\begin{lstlisting}
int __alloc(struct pcb_t *caller, int vmaid, int rgid, addr_t size,
			addr_t *alloc_addr)
{
	// First try to reuse free region
	if (get_free_vmrg_area(caller, vmaid, size, &rgnode) == 0) {
		*alloc_addr = rgnode.rg_start;
		return 0;
	}
	// Only expand heap if no free region available
}
		\end{lstlisting}
	\end{enumerate}
\end{enumerate}
In summary, the Simple OS's segmentation-with-paging approach (VM areas + page tables) provides logical memory organization and eliminates external fragmentation, but at the cost of increased translation complexity, memory overhead for management structures, O(n) overlap validation, complex coordination between segment and page-level operations. The hybrid system requires maintaining both VM area metadata and page table structures, with operations spanning both abstraction levels, making the implementation more complex than a pure paging system.

\subsection{Question 7}
\textbf{Question:} What happens if the synchronization is not handled in your Simple OS? Illustrate the problem of your simple OS (assignment outputs) by example if you have any.

\vspace{0.5cm}

\noindent\textbf{Answer:}

The Simple OS uses a multi-threaded architecture where multiple CPU threads (from \texttt{cpu\_routine()} in \texttt{os.c}) and a loader thread (from \texttt{ld\_routine()}) execute concurrently, sharing critical data structures. Without proper synchronization, severe system failures would occur.

% Bắt đầu danh sách cấp 1 (1., 2., 3.)
\begin{enumerate}[label=\textbf{\arabic*.}]
	
	% --- 1. Synchronization Mechanisms ---
	\item \textbf{Synchronization Mechanisms in Simple OS}
	
	The OS implements three main synchronization mechanisms:
	\begin{itemize}
		\item \textbf{Timer Synchronization} (\texttt{timer.c}): Uses mutexes (\texttt{event\_lock}, \texttt{timer\_lock}) and condition variables (\texttt{event\_cond}, \texttt{timer\_cond}) to coordinate time slot advancement. The \texttt{timer\_routine()} waits for all devices to complete before incrementing \texttt{\_time}, and \texttt{next\_slot()} synchronizes CPU/loader threads with the timer.
		
		\item \textbf{Scheduler Synchronization} (\texttt{sched.c}): Uses \texttt{queue\_lock} mutex to protect shared scheduler state including \texttt{mlq\_ready\_queue[]}, \texttt{slot\_remaining[]}, \texttt{current\_queue\_index}, and \texttt{running\_list}. Functions like \texttt{get\_mlq\_proc()}, \texttt{put\_mlq\_proc()}, and \texttt{add\_mlq\_proc()} are all protected by this mutex.
		
		\item \textbf{Memory Management Synchronization} (\texttt{libmem.c}): Uses \texttt{mmvm\_lock} mutex to protect VM area structures and page table operations. Functions like \texttt{\_\_alloc()}, \texttt{\_\_write()}, and \texttt{\_\_read()} acquire this lock before accessing memory management structures.
	\end{itemize}
	
	% --- 2. Consequences of Missing Synchronization ---
	\item \textbf{Consequences of Missing Synchronization}
	
	\begin{enumerate}[label=\textbf{\alph*.}]
		\item \textbf{Ready Queue Corruption:} 
		Without \texttt{queue\_lock} protection, multiple CPU threads could simultaneously call \texttt{get\_mlq\_proc()} and \texttt{put\_mlq\_proc()}, leading to race conditions on \texttt{mlq\_ready\_queue[]}. This could result in:
		\begin{itemize}
			\item \textbf{Lost Processes:} A process dequeued by one CPU thread might be simultaneously dequeued by another, causing one CPU to receive \texttt{NULL} or an invalid process pointer.
			\item \textbf{Duplicate Process Execution:} The same process could be dispatched to multiple CPUs simultaneously, leading to inconsistent state and data corruption.
			\item \textbf{Queue Structure Corruption:} Concurrent enqueue/dequeue operations could corrupt the queue's internal structure (size, pointers), causing segmentation faults or infinite loops.
		\end{itemize}
		
		\item \textbf{Scheduler State Inconsistency:}
		The scheduler maintains state variables \texttt{slot[]} and \texttt{mlq\_ready\_queue} that must be updated atomically. Without synchronization:
		\begin{itemize}
			\item \textbf{Incorrect Priority Scheduling:} Multiple threads reading and updating \texttt{mlq\_ready\_queue} simultaneously could cause the scheduler to skip priority levels or process them out of order, violating the MLQ scheduling policy.
			\item \textbf{Slot Counter Corruption:} The \texttt{slot[MAX\_PRIO]} counter could be incorrectly decremented by multiple threads, causing processes to receive incorrect time slices or be prematurely moved to lower priority queues.
		\end{itemize}
		
		\item \textbf{Timer Desynchronization:}
		The timer mechanism in \texttt{timer\_routine()} coordinates all threads by waiting for each device's \texttt{done} flag before advancing time slots. Without proper mutex protection:
		\begin{itemize}
			\item \textbf{Time Slot Skips:} If multiple threads call \texttt{next\_slot()} simultaneously, the timer might advance before all threads complete their work, causing some threads to miss time slots or execute out of order.
			\item \textbf{Deadlock in Timer:} The condition variable waits in \texttt{timer\_routine()} (lines 18-20 below) and \texttt{next\_slot()} (lines 61-63 below) could deadlock if mutexes are not properly acquired, freezing the entire system.
			\item \textbf{Inconsistent Time Perception:} Different CPU threads might observe different values of \texttt{current\_time()} if the timer advances while threads are reading it, leading to incorrect process scheduling decisions.
			\begin{lstlisting}[language=C, caption=timer\_routine() and next\_slot() in timer.c]
// timer.c
static void *timer_routine(void *args)
{
	while (!timer_stop)
	{
		printf("Time slot %3llu\n", current_time());
		int fsh = 0;
		int event = 0;
		/* Wait for all devices have done the job in current
		* time slot */
		struct timer_id_container_t *temp;
		for (temp = dev_list; temp != NULL; temp = temp->next)
		{
			pthread_mutex_lock(&temp->id.event_lock);
			while (!temp->id.done && !temp->id.fsh)
			{
				pthread_cond_wait(
				&temp->id.event_cond,
				&temp->id.event_lock);
			}
			if (temp->id.fsh)
			{
				fsh++;
			}
			event++;
			pthread_mutex_unlock(&temp->id.event_lock);
		}
		
		/* Increase the time slot */
		_time++;
		
		/* Let devices continue their job */
		for (temp = dev_list; temp != NULL; temp = temp->next)
		{
			pthread_mutex_lock(&temp->id.timer_lock);
			temp->id.done = 0;
			pthread_cond_signal(&temp->id.timer_cond);
			pthread_mutex_unlock(&temp->id.timer_lock);
		}
		if (fsh == event)
		{
			break;
		}
	}
	pthread_exit(args);
}

void next_slot(struct timer_id_t *timer_id)
{
	/* Tell to timer that we have done our job in current slot */
	pthread_mutex_lock(&timer_id->event_lock);
	timer_id->done = 1;
	pthread_cond_signal(&timer_id->event_cond);
	pthread_mutex_unlock(&timer_id->event_lock);
	
	/* Wait for going to next slot */
	pthread_mutex_lock(&timer_id->timer_lock);
	while (timer_id->done)
	{
		pthread_cond_wait(
		&timer_id->timer_cond,
		&timer_id->timer_lock);
	}
	pthread_mutex_unlock(&timer_id->timer_lock);
}
			\end{lstlisting}
		\end{itemize}
		
		\item \textbf{Memory Management Corruption:}
		Without \texttt{mmvm\_lock} protection, concurrent memory operations could corrupt critical structures:
		\begin{itemize}
			\item \textbf{VM Area List Corruption:} Multiple threads accessing \texttt{mm\_struct->mmap} (the VM area linked list) simultaneously could corrupt list pointers, causing processes to access wrong memory regions or crash when traversing the list.
			\item \textbf{Page Table Race Conditions:} Concurrent calls to \texttt{pte\_set\_fpn()} and \texttt{pte\_get\_entry()} could result in reading partially updated page table entries, causing incorrect address translations and memory access violations.
			\item \textbf{Symbol Table Corruption:} The \texttt{symrgtbl[]} array could be corrupted if multiple threads allocate memory regions simultaneously, leading to processes accessing incorrect memory addresses or overwriting each other's data.
		\end{itemize}
		
		\item \textbf{Process State Corruption:}
		In \texttt{cpu\_routine()}, each CPU thread maintains local state (\texttt{proc}, \texttt{time\_left}) but accesses shared process control blocks. Without synchronization:
		\begin{itemize}
			\item \textbf{Concurrent Process Execution:} The same \texttt{pcb\_t} structure could be modified by multiple CPU threads simultaneously, corrupting the program counter (\texttt{pc}), registers, or memory mappings.
			\item \textbf{Inconsistent Process Termination:} A process might be freed by one CPU thread while another is still executing it, leading to use-after-free errors and system crashes.
		\end{itemize}
		
		\item \textbf{Loader-CPU Race Conditions:}
		The \texttt{ld\_routine()} adds processes to the ready queue via \texttt{add\_proc()} while CPU threads are simultaneously removing processes via \texttt{get\_proc()}. Without synchronization:
		\begin{itemize}
			\item \textbf{Process Addition During Dequeue:} A process might be added to the queue while another thread is in the middle of dequeuing, causing the new process to be lost or the dequeue operation to fail.
			\item \textbf{Incomplete Process Initialization:} A CPU thread might dispatch a process before the loader finishes initializing its memory structures (\texttt{mm\_struct}), leading to null pointer dereferences or uninitialized memory access.
		\end{itemize}
	\end{enumerate}
	
	% --- 3. System-Wide Failures ---
	\item \textbf{System-Wide Failures}
	
	Without synchronization, the Simple OS would experience cascading failures:
	\begin{itemize}
		\item \textbf{Non-Deterministic Behavior:} The same input could produce different outputs on each run due to race conditions, making debugging and testing extremely difficult.
		\item \textbf{System Crashes:} Segmentation faults from corrupted pointers, null pointer dereferences, or invalid memory accesses would cause the entire OS simulation to crash.
		\item \textbf{Resource Leaks:} Lost processes or corrupted queue structures could prevent proper cleanup, leading to memory leaks and resource exhaustion.
		\item \textbf{Incorrect Execution Results:} Even if the system doesn't crash, processes might execute incorrectly due to corrupted state, producing wrong outputs that are difficult to detect.
	\end{itemize}
	
	% --- 4. Experimental Demonstration ---
	\item \textbf{Experimental Demonstration: Removing Timer Synchronization}
	
	To demonstrate the consequences of missing synchronization, we removed all mutex protection from \texttt{timer\_routine()} (\texttt{timer.c}) and \texttt{next\_slot()} functions. This eliminates the critical synchronization mechanism that coordinates time slot advancement across all CPU threads and the loader thread.
	
	\begin{enumerate}[label=\textbf{\alph*.}]
		\item \textbf{Experimental Setup}
		
		The mutexes \texttt{event\_lock} and \texttt{timer\_lock} (defined per device in \texttt{struct timer\_id\_t}), along with their associated condition variables (\texttt{event\_cond} and \texttt{timer\_cond}), were removed from the timer synchronization code. Specifically:
		\begin{itemize}
			\item In \texttt{timer\_routine()}: Removed mutex locks at lines 32, 44, 53, 56 that protect the \texttt{done} flag checks and updates
			\begin{lstlisting}[language=C, caption=Removed muxtex timer\_routine()]
// timer.c
static void *timer_routine(void *args)
{
	/*...*/
	for (temp = dev_list; temp != NULL; temp = temp->next)
	{
		//line 32	pthread_mutex_lock(&temp->id.event_lock);
		while (!temp->id.done && !temp->id.fsh)
		{
			pthread_cond_wait(
			&temp->id.event_cond,
			&temp->id.event_lock);
		}
		if (temp->id.fsh)
		{
			fsh++;
		}
		event++;
		//line 44	pthread_mutex_unlock(&temp->id.event_lock);
	}
	/*...*/
	for (temp = dev_list; temp != NULL; temp = temp->next)
	{
		//line 53	pthread_mutex_lock(&temp->id.timer_lock);
		temp->id.done = 0;
		pthread_cond_signal(&temp->id.timer_cond);
		//line 56	pthread_mutex_unlock(&temp->id.timer_lock);
	}
	/*...*/
}
			\end{lstlisting}
			\item In \texttt{next\_slot()}: Removed mutex locks at lines 69, 72, 75, 82 that coordinate between threads and the timer
			\begin{lstlisting}[language=C, caption=Removed muxtex next\_slot()]
// timer.c
void next_slot(struct timer_id_t *timer_id)
{
	/* Tell to timer that we have done our job in current slot */
	//line 69	pthread_mutex_lock(&timer_id->event_lock);
	timer_id->done = 1;
	pthread_cond_signal(&timer_id->event_cond);
	//line 72	pthread_mutex_unlock(&timer_id->event_lock);
	
	/* Wait for going to next slot */
	//line 75	pthread_mutex_lock(&timer_id->timer_lock);
	while (timer_id->done)
	{
		pthread_cond_wait(
		&timer_id->timer_cond,
		&timer_id->timer_lock);
	}
	//line 82	pthread_mutex_unlock(&timer_id->timer_lock);
}
			\end{lstlisting}
		\end{itemize}
		This allows multiple threads (CPU threads from \texttt{cpu\_routine()} in \texttt{os.c} and the loader thread from \texttt{ld\_routine()}) to access and modify the timer state (\texttt{\_time} of \texttt{timer.c}) and device \texttt{done} flags concurrently without any protection.
		
		\item \textbf{Observed Output}
		
		The following output was captured from a test run with timer synchronization removed:
		
		\begin{lstlisting}
Time slot   0
ld_routine
Loaded a process at input/proc/p0s, PID: 1 PRIO: 0

Time slot   1
CPU 0: Dispatched process  1
Loaded a process at input/proc/p1s, PID: 2 PRIO: 15
CPU 1: Dispatched process  2

Time slot   2
liballoc:152
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab8001000 P4D=0x7fbab8003000 PUD=0x7fbab8005000 PMD=0x7fbab8007000 PT=0x7fbab8009000
0000000000000000: 0000000000000001

Time slot   3
liballoc:152
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab8001000 P4D=0x7fbab8003000 PUD=0x7fbab8005000 PMD=0x7fbab8007000 PT=0x7fbab8009000
0000000000000000: 0000000000000001
Loaded a process at input/proc/p1s, PID: 3 PRIO: 0

Time slot   4
libfree:168
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab8001000 P4D=0x7fbab8003000 PUD=0x7fbab8005000 PMD=0x7fbab8007000 PT=0x7fbab8009000
0000000000000000: 0000000000000001

Time slot   5
liballoc:152
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab8001000 P4D=0x7fbab8003000 PUD=0x7fbab8005000 PMD=0x7fbab8007000 PT=0x7fbab8009000
0000000000000000: 0000000000000001

Time slot   6
libwrite:343
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab8001000 P4D=0x7fbab8003000 PUD=0x7fbab8005000 PMD=0x7fbab8007000 PT=0x7fbab8009000
0000000000000000: 0000000000000001
Loaded a process at input/proc/p0s, PID: 4 PRIO: 0
CPU 0: Put process  1 to run queue
CPU 1: Put process  2 to run queue
CPU 1: Dispatched process  3
CPU 0: Dispatched process  4

Time slot   7

Time slot   8
liballoc:152
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab800b000 P4D=0x7fbab800d000 PUD=0x7fbab800f000 PMD=0x7fbab8011000 PT=0x7fbab8013000
0000000000000000: 0000000000001001

Time slot   9
liballoc:152
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab800b000 P4D=0x7fbab800d000 PUD=0x7fbab800f000 PMD=0x7fbab8011000 PT=0x7fbab8013000
0000000000000000: 0000000000001001

Time slot   10
libfree:168
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab800b000 P4D=0x7fbab800d000 PUD=0x7fbab800f000 PMD=0x7fbab8011000 PT=0x7fbab8013000
0000000000000000: 0000000000001001

Time slot   11
liballoc:152
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab800b000 P4D=0x7fbab800d000 PUD=0x7fbab800f000 PMD=0x7fbab8013000
0000000000000000: 0000000000001001

Time slot   12
libwrite:343
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab800b000 P4D=0x7fbab800d000 PUD=0x7fbab800f000 PMD=0x7fbab8011000 PT=0x7fbab8013000
0000000000000000: 0000000000001001

Time slot   13
CPU 1: Put process  3 to run queue
CPU 1: Dispatched process  1
libread:304
CPU 0: Put process  4 to run queue
CPU 0: Dispatched process  3

Time slot   14

Time slot   15

Time slot   16

Time slot   17
CPU 0: Processed  3 has finished
CPU 0: Dispatched process  4
libread:304

Time slot   18

Time slot   19
CPU 1: Put process  1 to run queue
CPU 1: Dispatched process  1
libfree:168
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab8001000 P4D=0x7fbab8003000 PUD=0x7fbab8005000 PMD=0x7fbab8007000 PT=0x7fbab8009000
0000000000000000: 0000000000000001

Time slot   20

Time slot   21
CPU 1: Processed  1 has finished
CPU 1: Dispatched process  2

Time slot   22

Time slot   23
CPU 0: Put process  4 to run queue
CPU 0: Dispatched process  4
libfree:168
--- Page Table Dump ---
print_pgtbl:
PGD=0x7fbab800b000 P4D=0x7fbab800d000 PUD=0x7fbab800f000 PMD=0x7fbab8011000 PT=0x7fbab8013000
0000000000000000: 0000000000001001

Time slot   24

Time slot   25
CPU 0: Processed  4 has finished
CPU 1: Processed  2 has finished
CPU 0 stopped
CPU 1 stopped
		\end{lstlisting}
		
		\item \textbf{Analysis of the Results}
		
		While this particular run completed successfully, several critical observations highlight the dangers of missing synchronization:
		
		\begin{itemize}
			\item \textbf{Concurrent Operations in Same Time Slot:} At time slot 6, we observe multiple events occurring simultaneously within the same time slot: \texttt{libwrite:343}, process loading (PID 4), CPU 0 putting process 1 to run queue, CPU 1 putting process 2 to run queue, and both CPUs dispatching new processes (3 and 4). Without timer synchronization, the loader thread, CPU 0, and CPU 1 are all executing concurrently. The \texttt{next\_slot()} function (called from \texttt{cpu\_routine()} at lines 66, 99, 112 and from \texttt{ld\_routine()} at lines 140, 155 in \texttt{os.c}) should coordinate with \texttt{timer\_routine()} to ensure only one time slot is active at a time. Without mutexes, threads can read and modify the \texttt{done} flag and \texttt{\_time} variable concurrently, causing operations from different logical time slots to interleave.
			
			\item \textbf{Potential Page Table Race Conditions:} While the page table dumps appear consistent in this run, without proper synchronization, concurrent access to page table structures could lead to race conditions. Multiple threads could simultaneously call \texttt{pte\_set\_fpn()} or \texttt{pte\_get\_entry()} (in \texttt{mm64.c}), which traverse the 5-level page table hierarchy via \texttt{get\_page\_table\_entry()}. Without mutex protection, one thread could be updating a page table entry (via \texttt{get\_next\_level()} allocating new directory levels) while another thread reads it, potentially observing partially updated pointers or inconsistent states. The fact that this run shows consistent page tables does not guarantee correctness—race conditions are non-deterministic and may manifest differently in subsequent runs.
			
			\item \textbf{Concurrent Memory Operations:} At time slot 13, we see both CPUs performing operations simultaneously: CPU 1 putting process 3 to run queue and dispatching process 1, CPU 1 performing \texttt{libread:304}, CPU 0 putting process 4 to run queue and dispatching process 3. CPU 1 is reading memory while CPU 0 is dispatching a process. Without proper synchronization, these operations could interfere with each other, especially if they access shared scheduler structures or memory management data.
			
			\item \textbf{Non-Deterministic Execution Order:} The output shows that operations from different threads can appear in any order within a time slot. For example, at time slot 6, the process loading, CPU queue operations, and dispatches all occur in an unpredictable sequence. This non-determinism makes it impossible to reproduce bugs consistently, as the exact interleaving depends on thread scheduling by the operating system.
			
			\item \textbf{Time Slot Coordination Failure:} The \texttt{timer\_routine()} function (lines 20-64 of \texttt{timer.c}) is designed to:
			\begin{enumerate}
				\item Wait for all devices to set their \texttt{done} flag (lines 30-45)
				\item Increment \texttt{\_time} (line 48)
				\item Signal all devices to continue (lines 51-57)
			\end{enumerate}
			Without mutex protection, multiple threads can call \texttt{next\_slot()} simultaneously, and the timer thread can advance \texttt{\_time} while other threads are still reading it or setting their \texttt{done} flags. This breaks the atomicity of time slot transitions, causing threads to operate on inconsistent time values.
			
			\item \textbf{Ambiguous Results:} The fact that this run completed with no errors demonstrates a critical characteristic of race conditions: they are \textbf{non-deterministic}. The absence of visible errors in one run does not guarantee correctness. While this particular run shows consistent page table structures and no obvious data corruption, the concurrent operations observed (especially at time slots 6 and 13) indicate that threads are executing without proper coordination. Subsequent runs with the same input might:
			\begin{itemize}
				\item Crash due to corrupted pointers or invalid memory accesses
				\item Produce incorrect results due to data corruption
				\item Deadlock if threads wait on condition variables without proper mutex protection (the condition variable waits in \texttt{timer\_routine()} and \texttt{next\_slot()} require mutexes to function correctly)
				\item Exhibit different execution orders, making debugging and testing impossible
			\end{itemize}
		\end{itemize}
		
		\item \textbf{Why This Demonstrates the Problem}
		
		The removal of timer synchronization mutexes creates a \textbf{time-of-check to time-of-use (TOCTOU)} vulnerability and breaks the atomicity of time slot transitions. The synchronization protocol in \texttt{timer.c} works as follows:
		
		\begin{enumerate}
			\item Each device thread calls \texttt{next\_slot()} which sets \texttt{done = 1} and signals the timer (lines 69-72)
			\item The device then waits for the timer to advance (lines 75-82)
			\item The timer waits for all devices to set \texttt{done = 1} (lines 30-45)
			\item The timer increments \texttt{\_time} and resets all \texttt{done} flags (lines 48, 54-56)
			\item The timer signals all devices to continue (line 55)
		\end{enumerate}
		
		Without mutex protection, this protocol breaks:
		\begin{itemize}
			\item Thread A might read \texttt{\_time = 5} and set \texttt{done = 1}
			\item Thread B might read \texttt{\_time = 5} and also set \texttt{done = 1}
			\item The timer thread might increment \texttt{\_time} to 6 while Thread A is still in the middle of its operation
			\item Thread A completes its operation thinking it's in time slot 5, but the timer has already advanced
			\item Multiple threads can observe different values of \texttt{\_time} simultaneously
		\end{itemize}
	\end{enumerate}
	The concurrent operations observed at time slots 6 and 13 provide concrete evidence of this race condition: multiple threads are executing operations simultaneously without proper coordination, breaking the atomicity of time slot transitions.
	
	
\end{enumerate}

\vspace{0.5cm}
In summary, synchronization is essential for the Simple OS's correctness. The mutex-protected critical sections in \texttt{timer.c} (protecting \texttt{\_time} and \texttt{done} flags), \texttt{sched.c} (protecting queue operations), and \texttt{libmem.c} (protecting memory management structures) ensure that shared data structures are accessed atomically, preventing race conditions, data corruption, and system failures. The experimental removal of timer synchronization demonstrates that even when a run appears successful, the underlying race conditions create non-deterministic behavior. The concurrent operations observed in the output show that threads are not properly coordinated, which can lead to subtle bugs, data corruption, and unpredictable system behavior in subsequent runs.


    \pagebreak
    
\section{Overall}

In this assignment, we undertook the design and implementation of the core components for a simple Operating System simulation, focusing on a Symmetric Multiprocessing (SMP) environment. The project integrated three critical modules: a Slot-based Scheduler, a Hybrid Memory Management system, and a Synchronization mechanism.

For process management, we implemented a Slot-based Multi-Level Queue (MLQ) scheduling algorithm. Unlike standard priority scheduling which is prone to starvation, our design introduces a dynamic slot mechanism ($slot = MAX\_PRIO - prio$) combined with Round-Robin execution within each queue. This approach is designed to balance honoring process priority with maintaining fairness, aiming to allow lower-priority tasks to eventually receive CPU resources even under heavy load. The implementation addresses the logic of context switching and load balancing across multiple simulated CPUs.

Regarding memory management, we constructed a hybrid architecture that combines Segmentation with 5-Level Paging. A focal point of this implementation is the simulation of the hardware page walk mechanism. We programmed the traversal logic that iteratively resolves a 64-bit virtual address through the five hierarchical levels: PGD $\rightarrow$ P4D $\rightarrow$ PUD $\rightarrow$ PMD $\rightarrow$ PT. By utilizing bit-masking and shifting operations to extract directory indices at each level, our system replicates the translation process of modern MMUs. This mechanism is intended to enable the OS to handle sparse memory allocation within a massive 128 PiB address space while establishing isolation between user and kernel modes.

Furthermore, we addressed the challenge of concurrency in a multi-threaded architecture. We identified potential race conditions that could lead to data corruption or system instability when multiple CPUs and the loader access shared resources simultaneously. By applying synchronization mechanisms, specifically Mutex locks on the ready queue, memory structures, and the global timer, we sought to preserve the atomicity of critical operations and promote the deterministic behavior of the system.

In conclusion, this assignment has provided a comprehensive hands-on experience in building OS kernel components. It bridged the gap between theoretical concepts, such as virtual memory translation, process scheduling, thread safety and practical system programming. The insights gained from handling the complexity of the 5-level page walking algorithm and SMP synchronization serve as a foundation for understanding the architecture of modern, high-performance operating systems.

\section{Source Code}

\begin{enumerate}
    \item \textbf{Scheduler:} \href{https://github.com/quocthangtrann/os_lamiaatrium-2.git}{Link GitHub}
    \item \textbf{Memory Management:} \href{https://github.com/tom1209-netizen/lamia_atrium.git}{Link GitHub}
\end{enumerate}

\end{document}





